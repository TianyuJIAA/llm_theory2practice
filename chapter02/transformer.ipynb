{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大语言模型基础\n",
    "\n",
    "本章将首先介绍 Transformer 结构，并在此基础上介绍生成式预训练语言模型 GPT、大语言模型网络结构和注意力机制优化以及相关实践。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer 模型\n",
    "\n",
    "Transformer结构及代码这里不再赘述，放一张结构图即可  \n",
    "\n",
    "<img src=\"./images/transformer.png\" style=\"zoom:60%;\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成式语言模型GPT\n",
    "\n",
    "GPT是由OPENAI提出的一种生成式预训练语言模型，由多层Transformer组成(只包含Decoder)的单向语言模型，其结构为:  \n",
    "\n",
    "<img src=\"./images/gpt.png\" style=\"zoom:70%;\" />  \n",
    "\n",
    "GPT的模型的训练过程主要包括: 无监督预训练和有监督下游任务微调  \n",
    "\n",
    "#### 无监督预训练\n",
    "\n",
    "GPT 采用生成式预训练方法，单向意味着模型只能从左到右或从右到左对文本序列建模，所采用的`Transformer`结构和解码策略保证了输入文本每个位置只能依赖过去时刻的信息。\n",
    "\n",
    "#### 有监督下游任务微调\n",
    "\n",
    "通过无监督语言模型预训练，使得GPT模型具备了一定的通用语义表示能力。下游任务微调（Downstream Task Fine-tuning）的目的是在通用语义表示基础上，根据下游任务的特性进行适配。下游任务通常需要利用有标注数据集进行训练，每个样例由输入长度为n的文本序列和对应的标签y构成。\n",
    "\n",
    "下游任务在微调过程中，针对任务目标进行优化，很容易使得模型遗忘预训练阶段所学习到的通用语义知识表示，从而损失模型的通用性和泛化能力，造成灾难性遗忘（Catastrophic Forgetting）问题。因此，通常会采用混合预训练任务损失和下游微调损失的方法来缓解上述问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预训练语言模型实践\n",
    "\n",
    "通过HuggingFace提供的数据集，以Bert模型为例，构建预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 准备数据集\n",
    "\n",
    "书中是直接通过代码下载HuggingFace的数据集，但是实际操作中数据集很大而且下载很慢，尽管配置了[hf-mirror](https://hf-mirror.com/)的镜像源速度也不快。所以最后选择从官网直接下载数据集，然后本地直接load就ok了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/bert_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2000000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'url', 'title', 'text'],\n",
      "    num_rows: 1250308\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 直接通过代码下载\n",
    "#bookcorpus = load_dataset(\"bookcorpus\", split=\"train\", trust_remote_code=True)\n",
    "#wiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "# 先从官网下载，然后本地load wikipedia数据集比较大只下载了部分数据\n",
    "bookcorpus = load_dataset(path=\"data/bookcorpus\", split=\"train\")\n",
    "bookcorpus = bookcorpus.select(range(2000000))  \n",
    "print(bookcorpus)\n",
    "wiki = load_dataset(path='data/wikimedia/wikipedia', split=\"train\")\n",
    "print(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 3250308\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2925277\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 325031\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 拼接数据集，同时将数据集保存到本地文件中\n",
    "\n",
    "# 仅保留 'text' 列\n",
    "wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])\n",
    "dataset = concatenate_datasets([bookcorpus, wiki])\n",
    "print(dataset)\n",
    "\n",
    "# 拆分并将数据集保存到本地文件中\n",
    "d = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "print(d)\n",
    "\n",
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        for t in dataset[\"text\"]:\n",
    "            print(t, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the training set to train.txt\n",
    "dataset_to_text(d[\"train\"], \"data/train.txt\")\n",
    "#save the testing set to test.txt\n",
    "dataset_to_text(d[\"test\"], \"data/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 训练词元分析器(Tokenizer)\n",
    "\n",
    "BERT采用了`WordPiece`分词，根据训练语料中的词频决定是否将一个完整的词切分为多个词元。因此，需要首先训练词元分析器(Tokenizer)。可以使用transformers库中的`BertWordPieceTokenizer`类来完成任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]]', '[MASK]', '<S>', '<T>']\n",
    "\n",
    "files = ['data/train.txt']\n",
    "vocab_size = 30_522\n",
    "max_length = 512\n",
    "trauncate_longer_samples = False\n",
    "\n",
    "# 初始化分词器\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "# train\n",
    "tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "tokenizer.enable_truncation(max_length=max_length)\n",
    "\n",
    "model_path = \"pretrained-bert\"\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    os.mkdir(model_path)\n",
    "\n",
    "tokenizer.save_model(model_path)\n",
    "\n",
    "# dumping some of the tokenizer config to config file,\n",
    "# including special tokens, whether to lower case and the maximum sequence length\n",
    "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
    "    tokenizer_cfg = {\n",
    "    \"do_lower_case\": True,\n",
    "    \"unk_token\": \"[UNK]\",\n",
    "    \"sep_token\": \"[SEP]\",\n",
    "    \"pad_token\": \"[PAD]\",\n",
    "    \"cls_token\": \"[CLS]\",\n",
    "    \"mask_token\": \"[MASK]\",\n",
    "    \"model_max_length\": max_length,\n",
    "    \"max_len\": max_length,\n",
    "    }\n",
    "    json.dump(tokenizer_cfg, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 预处理语料\n",
    "\n",
    "将训练集和测试集通过训练好的Tokenizer进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# 加载Tokenizer\n",
    "max_length = 512\n",
    "model_path = \"pretrained-bert\"\n",
    "trauncate_longer_samples = False\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "def encode_with_truncation(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=\"max_length\",\n",
    "                     max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_trauncation(examples):\n",
    "    return tokenizer(examples['text'], return_special_tokens_mask=True)\n",
    "\n",
    "# 根据trauncate_longer_samples参数确定是否对数据集进行截断\n",
    "encode = encode_with_truncation if trauncate_longer_samples else encode_without_trauncation\n",
    "\n",
    "# 处理数据集\n",
    "# map函数具体可以参照huggingface官网: https://huggingface.co/docs/datasets/about_map_batch\n",
    "# batch_size默认为1000\n",
    "train_dataset = d['train'].map(encode, batched=True)\n",
    "test_dataset = d['test'].map(encode, batched=True)\n",
    "\n",
    "print(\"train_dataset\", train_dataset)\n",
    "print(\"test_dataset\", test_dataset)\n",
    "\n",
    "if trauncate_longer_samples:\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "    # 如果不截断的话需要保留特殊的token ['SEP'] ['CLS']等\n",
    "    test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "    train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "\n",
    "print(\"train_dataset\", train_dataset)\n",
    "print(\"test_dataset\", test_dataset)\n",
    "\n",
    "# 因为设置了不截断，所以需要将样本连接起来，组合成固定长度的向量\n",
    "from itertools import chain\n",
    "\n",
    "def group_texts(examples):\n",
    "    # concat all texts\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # 这里trauncate了最后一块数据\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i: i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "if not trauncate_longer_samples:\n",
    "    train_dataset = train_dataset.map(group_texts, batched=True, desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    test_dataset = test_dataset.map(group_texts, batched=True, desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    \n",
    "    # convert them from lists to torch tensors\n",
    "    train_dataset.set_format(\"torch\")\n",
    "    test_dataset.set_format(\"torch\")\n",
    "\n",
    "print(\"train_dataset\", train_dataset)\n",
    "print(\"test_dataset\", test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# 初始化模型\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)\n",
    "\n",
    "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens\n",
    "# for the Masked Language Modeling (MLM) task\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    overwrite_output_dir=False,\n",
    "    num_train_epochs=5,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_gpu_eval_batch_size=64,\n",
    "    logging_steps=1000,\n",
    "    save_on_each_node=1000,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(os.path.join(model_path, ''))\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# perform predictions\n",
    "examples = [\n",
    "    \"Today's most trending hashtags on [MASK] is Donald Trump\",\n",
    "    \"The [MASK] was cloudy yesterday, but today it's rainy.\", \n",
    "    ]\n",
    "\n",
    "for example in examples:\n",
    "    for prediction in fill_mask(example):\n",
    "        print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA及注意力机制优化\n",
    "\n",
    "#### LLaMA模型结构\n",
    "\n",
    "LLaMA结构整体与GPT-2类似，都是基于Transformer-Decoder的。不同的地方是：采用了前置归一化并使用RMS归一化函数、激活函数更换为SwiGLU，并使用了旋转位置嵌入，GPT-2结构如图:  \n",
    "\n",
    "<img src=\"./images/gpt2.png\" style=\"zoom: 70%;\" />  \n",
    "\n",
    "1.RMSNorm归一化函数  \n",
    "\n",
    "为了使得模型训练过程更加稳定，`GPT-2`相较于`GPT`就引入了前置层归一化方法，将第一个层归一化移动到多头自注意力层之前，第二个层归一化也移动到了全连接层之前，同时残差连接的位置也调整到了多头自注意力层与全连接层之后。层归一化中也采用了`RMSNorm`归一化函数。\n",
    "\n",
    "<img src=\"./images/RMSNorm.png\" style=\"zoom: 70%;\" />  \n",
    "\n",
    "\n",
    "2.SwiGLU 激活函数  \n",
    "\n",
    "取代了`ReLU`，在LLaMA中全连接层使用带有SwiGLU激活函数的`FFN`(Position-wise Feed-Forward Network):  \n",
    " \n",
    "<img src=\"./images/SwiGLU.png\" style=\"zoom: 70%;\" />  \n",
    "\n",
    "β取值不同时，Swish激活函数的形状也不同:  \n",
    "\n",
    "<img src=\"./images/Swish.png\" style=\"zoom: 70%;\" />  \n",
    "\n",
    "3.旋转位置嵌入(RoPE)  \n",
    "\n",
    "在位置编码上，使用旋转位置嵌入(Rotary Positional Embeddings，RoPE)代替原有的绝对位置编码。RoPE借助了`复数`的思想，出发点是通过绝对位置编码的方式实现相对位置编码。\n",
    "\n",
    "4.附上LLaMA不同模型规模下的具体超参数细节\n",
    "\n",
    "<img src=\"./images/LLaMA-weight.png\" style=\"zoom: 70%;\" />  \n",
    "\n",
    "最后附上HuggingFace Transformer库中LLaMA解码器整体实现代码实现:  \n",
    "\n",
    "```python\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.self_attn = LlamaAttention(config=config)\n",
    "        self.mlp = LlamaMLP(\n",
    "            hidden_size=self.hidden_size,\n",
    "            intermediate_size=config.intermediate_size,\n",
    "            hidden_act=config.hidden_act,\n",
    "            )\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = False, \n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        outputs = (hidden_states,)\n",
    "        \n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights,)\n",
    "        \n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "        \n",
    "        return outputs\n",
    "```\n",
    "\n",
    "#### 注意力机制优化\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
