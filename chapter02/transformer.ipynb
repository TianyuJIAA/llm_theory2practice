{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大语言模型基础\n",
    "\n",
    "本章将首先介绍 Transformer 结构，并在此基础上介绍生成式预训练语言模型 GPT、大语言模型网络结构和注意力机制优化以及相关实践。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer 模型\n",
    "\n",
    "Transformer结构及代码这里不再赘述，放一张结构图即可  \n",
    "\n",
    "<img src=\"./images/transformer.png\" style=\"zoom:60%;\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成式语言模型GPT\n",
    "\n",
    "GPT是由OPENAI提出的一种生成式预训练语言模型，由多层Transformer组成(只包含Decoder)的单向语言模型，其结构为:  \n",
    "\n",
    "<img src=\"./images/gpt.png\" style=\"zoom:70%;\" />  \n",
    "\n",
    "GPT的模型的训练过程主要包括: 无监督预训练和有监督下游任务微调  \n",
    "\n",
    "#### 无监督预训练\n",
    "\n",
    "GPT 采用生成式预训练方法，单向意味着模型只能从左到右或从右到左对文本序列建模，所采用的`Transformer`结构和解码策略保证了输入文本每个位置只能依赖过去时刻的信息。\n",
    "\n",
    "#### 有监督下游任务微调\n",
    "\n",
    "通过无监督语言模型预训练，使得GPT模型具备了一定的通用语义表示能力。下游任务微调（Downstream Task Fine-tuning）的目的是在通用语义表示基础上，根据下游任务的特性进行适配。下游任务通常需要利用有标注数据集进行训练，每个样例由输入长度为n的文本序列和对应的标签y构成。\n",
    "\n",
    "下游任务在微调过程中，针对任务目标进行优化，很容易使得模型遗忘预训练阶段所学习到的通用语义知识表示，从而损失模型的通用性和泛化能力，造成灾难性遗忘（Catastrophic Forgetting）问题。因此，通常会采用混合预训练任务损失和下游微调损失的方法来缓解上述问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预训练语言模型实践\n",
    "\n",
    "通过HuggingFace提供的数据集，以Bert模型为例，构建预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 准备数据集\n",
    "\n",
    "书中是直接通过代码下载HuggingFace的数据集，但是实际操作中数据集很大而且下载很慢，尽管配置了[hf-mirror](https://hf-mirror.com/)的镜像源速度也不快。所以最后选择从官网直接下载数据集，然后本地直接load就ok了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset arrow/bookcorpus to /Users/jiatianyu/.cache/huggingface/datasets/arrow/bookcorpus-f73b20ad1e10e33c/0.0.0/74f69db2c14c2860059d39860b1f400a03d11bf7fb5a8258ca38c501c878c137...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HF google storage unreachable. Downloading and preparing it from source\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 2335.36it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 227.95it/s]\n",
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset arrow downloaded and prepared to /Users/jiatianyu/.cache/huggingface/datasets/arrow/bookcorpus-f73b20ad1e10e33c/0.0.0/74f69db2c14c2860059d39860b1f400a03d11bf7fb5a8258ca38c501c878c137. Subsequent calls will reuse this data.\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2000000\n",
      "})\n",
      "Downloading and preparing dataset parquet/wikipedia to /Users/jiatianyu/.cache/huggingface/datasets/parquet/wikipedia-1ac450b58ffe6672/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 962.66it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 91.92it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /Users/jiatianyu/.cache/huggingface/datasets/parquet/wikipedia-1ac450b58ffe6672/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n",
      "Dataset({\n",
      "    features: ['id', 'url', 'title', 'text'],\n",
      "    num_rows: 1250308\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 直接通过代码下载\n",
    "#bookcorpus = load_dataset(\"bookcorpus\", split=\"train\", trust_remote_code=True)\n",
    "#wiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "# 先从官网下载，然后本地load wikipedia数据集比较大只下载了部分数据\n",
    "bookcorpus = load_dataset(path=\"data/bookcorpus\", split=\"train\")\n",
    "bookcorpus = bookcorpus.select(range(2000000))  \n",
    "print(bookcorpus)\n",
    "wiki = load_dataset(path='data/wikimedia/wikipedia', split=\"train\")\n",
    "print(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 3250308\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 拼接数据集，同时将数据集保存到本地文件中\n",
    "\n",
    "# 仅保留 'text' 列\n",
    "wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])\n",
    "dataset = concatenate_datasets([bookcorpus, wiki])\n",
    "print(dataset)\n",
    "\n",
    "# 拆分并将数据集保存到本地文件中\n",
    "d = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        for t in dataset[\"text\"]:\n",
    "            print(t, file=f)\n",
    "# save the training set to train.txt\n",
    "dataset_to_text(d[\"train\"], \"data/train.txt\")\n",
    "# save the testing set to test.txt\n",
    "dataset_to_text(d[\"test\"], \"data/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 训练词元分析器(Tokenizer)\n",
    "\n",
    "BERT采用了`WordPiece`分词，根据训练语料中的词频决定是否将一个完整的词切分为多个词元。因此，需要首先训练词元分析器(Tokenizer)。可以使用transformers库中的`BertWordPieceTokenizer`类来完成任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
