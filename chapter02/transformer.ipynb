{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大语言模型基础\n",
    "\n",
    "本章将首先介绍 Transformer 结构，并在此基础上介绍生成式预训练语言模型 GPT、大语言模型网络结构和注意力机制优化以及相关实践。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer 模型\n",
    "\n",
    "Transformer结构及代码这里不再赘述，放一张结构图即可  \n",
    "\n",
    "<img src=\"./images/transformer.png\" style=\"zoom:60%;\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成式语言模型GPT\n",
    "\n",
    "GPT是由OPENAI提出的一种生成式预训练语言模型，由多层Transformer组成(只包含Decoder)的单向语言模型，其结构为:  \n",
    "\n",
    "<img src=\"./images/gpt.png\" style=\"zoom:70%;\" />  \n",
    "\n",
    "GPT的模型的训练过程主要包括: 无监督预训练和有监督下游任务微调  \n",
    "\n",
    "#### 无监督预训练\n",
    "\n",
    "GPT 采用生成式预训练方法，单向意味着模型只能从左到右或从右到左对文本序列建模，所采用的`Transformer`结构和解码策略保证了输入文本每个位置只能依赖过去时刻的信息。\n",
    "\n",
    "#### 有监督下游任务微调\n",
    "\n",
    "通过无监督语言模型预训练，使得GPT模型具备了一定的通用语义表示能力。下游任务微调（Downstream Task Fine-tuning）的目的是在通用语义表示基础上，根据下游任务的特性进行适配。下游任务通常需要利用有标注数据集进行训练，每个样例由输入长度为n的文本序列和对应的标签y构成。\n",
    "\n",
    "下游任务在微调过程中，针对任务目标进行优化，很容易使得模型遗忘预训练阶段所学习到的通用语义知识表示，从而损失模型的通用性和泛化能力，造成灾难性遗忘（Catastrophic Forgetting）问题。因此，通常会采用混合预训练任务损失和下游微调损失的方法来缓解上述问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预训练语言模型实践\n",
    "\n",
    "通过HuggingFace提供的数据集，以Bert模型为例，构建预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 准备数据集\n",
    "\n",
    "书中是直接通过代码下载HuggingFace的数据集，但是实际操作中数据集很大而且下载很慢，尽管配置了[hf-mirror](https://hf-mirror.com/)的镜像源速度也不快。所以最后选择从官网直接下载数据集，然后本地直接load就ok了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/bert_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2000000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'url', 'title', 'text'],\n",
      "    num_rows: 1250308\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 直接通过代码下载\n",
    "#bookcorpus = load_dataset(\"bookcorpus\", split=\"train\", trust_remote_code=True)\n",
    "#wiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "# 先从官网下载，然后本地load wikipedia数据集比较大只下载了部分数据\n",
    "bookcorpus = load_dataset(path=\"data/bookcorpus\", split=\"train\")\n",
    "bookcorpus = bookcorpus.select(range(2000000))  \n",
    "print(bookcorpus)\n",
    "wiki = load_dataset(path='data/wikimedia/wikipedia', split=\"train\")\n",
    "print(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 3250308\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2925277\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 325031\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 拼接数据集，同时将数据集保存到本地文件中\n",
    "\n",
    "# 仅保留 'text' 列\n",
    "wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])\n",
    "dataset = concatenate_datasets([bookcorpus, wiki])\n",
    "print(dataset)\n",
    "\n",
    "# 拆分并将数据集保存到本地文件中\n",
    "d = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "print(d)\n",
    "\n",
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        for t in dataset[\"text\"]:\n",
    "            print(t, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the training set to train.txt\n",
    "dataset_to_text(d[\"train\"], \"data/train.txt\")\n",
    "#save the testing set to test.txt\n",
    "dataset_to_text(d[\"test\"], \"data/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 训练词元分析器(Tokenizer)\n",
    "\n",
    "BERT采用了`WordPiece`分词，根据训练语料中的词频决定是否将一个完整的词切分为多个词元。因此，需要首先训练词元分析器(Tokenizer)。可以使用transformers库中的`BertWordPieceTokenizer`类来完成任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]]', '[MASK]', '<S>', '<T>']\n",
    "\n",
    "files = ['data/train.txt']\n",
    "vocab_size = 30_522\n",
    "max_length = 512\n",
    "trauncate_longer_samples = False\n",
    "\n",
    "# 初始化分词器\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "# train\n",
    "tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "tokenizer.enable_truncation(max_length=max_length)\n",
    "\n",
    "model_path = \"pretrained-bert\"\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    os.mkdir(model_path)\n",
    "\n",
    "tokenizer.save_model(model_path)\n",
    "\n",
    "# dumping some of the tokenizer config to config file,\n",
    "# including special tokens, whether to lower case and the maximum sequence length\n",
    "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
    "    tokenizer_cfg = {\n",
    "    \"do_lower_case\": True,\n",
    "    \"unk_token\": \"[UNK]\",\n",
    "    \"sep_token\": \"[SEP]\",\n",
    "    \"pad_token\": \"[PAD]\",\n",
    "    \"cls_token\": \"[CLS]\",\n",
    "    \"mask_token\": \"[MASK]\",\n",
    "    \"model_max_length\": max_length,\n",
    "    \"max_len\": max_length,\n",
    "    }\n",
    "    json.dump(tokenizer_cfg, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 预处理语料\n",
    "\n",
    "将训练集和测试集通过训练好的Tokenizer进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# 加载Tokenizer\n",
    "max_length = 512\n",
    "model_path = \"pretrained-bert\"\n",
    "trauncate_longer_samples = False\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "def encode_with_truncation(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=\"max_length\",\n",
    "                     max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_trauncation(examples):\n",
    "    return tokenizer(examples['text'], return_special_tokens_mask=True)\n",
    "\n",
    "# 根据trauncate_longer_samples参数确定是否对数据集进行截断\n",
    "encode = encode_with_truncation if trauncate_longer_samples else encode_without_trauncation\n",
    "\n",
    "# 处理数据集\n",
    "# map函数具体可以参照huggingface官网: https://huggingface.co/docs/datasets/about_map_batch\n",
    "# batch_size默认为1000\n",
    "train_dataset = d['train'].map(encode, batched=True)\n",
    "test_dataset = d['test'].map(encode, batched=True)\n",
    "\n",
    "print(\"train_dataset\", train_dataset)\n",
    "print(\"test_dataset\", test_dataset)\n",
    "\n",
    "if trauncate_longer_samples:\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "    # 如果不截断的话需要保留特殊的token ['SEP'] ['CLS']等\n",
    "    test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "    train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "\n",
    "print(\"train_dataset\", train_dataset)\n",
    "print(\"test_dataset\", test_dataset)\n",
    "\n",
    "# 因为设置了不截断，所以需要将样本连接起来，组合成固定长度的向量\n",
    "from itertools import chain\n",
    "\n",
    "def group_texts(examples):\n",
    "    # concat all texts\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # 这里trauncate了最后一块数据\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i: i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "if not trauncate_longer_samples:\n",
    "    train_dataset = train_dataset.map(group_texts, batched=True, desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    test_dataset = test_dataset.map(group_texts, batched=True, desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    \n",
    "    # convert them from lists to torch tensors\n",
    "    train_dataset.set_format(\"torch\")\n",
    "    test_dataset.set_format(\"torch\")\n",
    "\n",
    "print(\"train_dataset\", train_dataset)\n",
    "print(\"test_dataset\", test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# 初始化模型\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)\n",
    "\n",
    "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens\n",
    "# for the Masked Language Modeling (MLM) task\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    overwrite_output_dir=False,\n",
    "    num_train_epochs=5,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_gpu_eval_batch_size=64,\n",
    "    logging_steps=1000,\n",
    "    save_on_each_node=1000,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(os.path.join(model_path, ''))\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# perform predictions\n",
    "examples = [\n",
    "    \"Today's most trending hashtags on [MASK] is Donald Trump\",\n",
    "    \"The [MASK] was cloudy yesterday, but today it's rainy.\", \n",
    "    ]\n",
    "\n",
    "for example in examples:\n",
    "    for prediction in fill_mask(example):\n",
    "        print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA及注意力机制优化\n",
    "\n",
    "#### LLaMA模型结构\n",
    "\n",
    "LLaMA结构整体与GPT-2类似，都是基于Transformer-Decoder的。不同的地方是：采用了前置归一化并使用RMS归一化函数、激活函数更换为SwiGLU，并使用了旋转位置嵌入，GPT-2结构如图:  \n",
    "\n",
    "<img src=\"./images/gpt2.png\" style=\"zoom: 70%;\" />  \n",
    "\n",
    "1.RMSNorm归一化函数  \n",
    "\n",
    "为了使得模型训练过程更加稳定，`GPT-2`相较于`GPT`就引入了前置层归一化方法，将第一个层归一化移动到多头自注意力层之前，第二个层归一化也移动到了全连接层之前，同时残差连接的位置也调整到了多头自注意力层与全连接层之后。层归一化中也采用了`RMSNorm`归一化函数。\n",
    "\n",
    "<img src=\"./images/RMSNorm.png\" style=\"zoom: 70%;\" />  \n",
    "\n",
    "\n",
    "2.SwiGLU 激活函数  \n",
    "\n",
    "取代了`ReLU`，在LLaMA中全连接层使用带有SwiGLU激活函数的`FFN`(Position-wise Feed-Forward Network):  \n",
    " \n",
    "<img src=\"./images/SwiGLU.png\" style=\"zoom: 70%;\" />  \n",
    "\n",
    "β取值不同时，Swish激活函数的形状也不同:  \n",
    "\n",
    "<img src=\"./images/Swish.png\" style=\"zoom: 70%;\" />  \n",
    "\n",
    "3.旋转位置嵌入(RoPE)  \n",
    "\n",
    "在位置编码上，使用旋转位置嵌入(Rotary Positional Embeddings，RoPE)代替原有的绝对位置编码。RoPE借助了`复数`的思想，出发点是通过绝对位置编码的方式实现相对位置编码。\n",
    "\n",
    "4.附上LLaMA不同模型规模下的具体超参数细节\n",
    "\n",
    "<img src=\"./images/LLaMA-weight.png\" style=\"zoom: 70%;\" />  \n",
    "\n",
    "最后附上HuggingFace Transformer库中LLaMA解码器整体实现代码实现:  \n",
    "\n",
    "```python\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.self_attn = LlamaAttention(config=config)\n",
    "        self.mlp = LlamaMLP(\n",
    "            hidden_size=self.hidden_size,\n",
    "            intermediate_size=config.intermediate_size,\n",
    "            hidden_act=config.hidden_act,\n",
    "            )\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = False, \n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        outputs = (hidden_states,)\n",
    "        \n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights,)\n",
    "        \n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "        \n",
    "        return outputs\n",
    "```\n",
    "\n",
    "#### 注意力机制优化\n",
    "\n",
    "在Transformer结构中，自注意力机制的时间和存储复杂度与序列的长度呈平方的关系，因此占用了大量的计算设备内存和并消耗大量计算资源。因此需要优化自注意力机制的时空复杂度、增强计算效率。目前主要包括两类优化方法:从近似注意力出发，旨在减少注意力计算和内存需求；从计算加速设备本身的特性出发，研究如何更好利用硬件特性对注意力层进行高效计算。  \n",
    "\n",
    "##### 1.稀疏注意力机制\n",
    "\n",
    "通过对一些训练好的 Transformer 模型中的注意力矩阵进行分析发现，其中很多通常是稀疏的，因此可以通过限制Query-Key对的数量来减少计算复杂度。这类方法就称为稀疏注意力(Sparse Attention)机制。可以将稀疏化方法进一步分成两类：基于位置信息和基于内容。\n",
    "\n",
    "基于位置的稀疏注意力机制的基本类型如图所示，主要包括五种类型。而现有的注意力机制，通常是基于五种基本基于位置的稀疏注意力机制的复合模式。  \n",
    "\n",
    "<img src=\"./images/sparse_attention.png\" style=\"zoom: 70%;\" />  \n",
    "\n",
    "基于内容的稀疏注意力是根据输入数据来创建稀疏注意力，其中一种很简单的方法是选择和给定查询`Query`有很高相似度的键`Key`，比如通过`K-means`方法、局部敏感哈希方法。  \n",
    "\n",
    "##### 2.FlashAttention\n",
    "\n",
    "上面介绍的稀疏注意力主要是减少注意力的计算来考虑的，而FlashAttention则更关注`MAC`(memory access cost)，即通过利用GPU硬件中的特殊设计，针对全局内存和共享存储的`I/O速度`的不同，尽可能的避免`HBM`中读取或写入注意力矩阵。FlashAttention 目标是尽可能高效地使用`SRAM`来加快计算速度，避免从全局内存中读取和写入注意力矩阵。\n",
    "\n",
    "\n",
    "GPU显存分为全局内存（Global memory）、本地内存（Local memory）、共享内存（Shared memory，SRAM）、寄存器内（Register memory）、常量内存（Constant memory）、纹理内存（Texture memory）等六大类。其中全局内存、本地内存、共享内存和寄存器内存具有读写能力。全局内存和本地内存使用的高带宽显存（High Bandwidth Memory，HBM）位于板卡 RAM 存储芯片上，该部分内存容量很大。全局内存是所有线程都可以访问，而本地内存则只能当前线程访问。NVIDIA H100 中全局内存有 80GB 空间，其访问速度虽然可以达到 3.35TB/s，但是如果全部线程同时访问全局内存时，其平均带宽仍然很低。共享内存和寄存器位于 GPU 芯片上，因此容量很小，并且共享内存只有在同一个 GPU 线程块（Thread Block）内的线程才可以共享访问，而寄存器仅限于同一个线程内部才能访问。NVIDIA H100 中每个 GPU 线程块在流式多处理器（Stream Multi-processor，SM）可以使用的共享存储容量仅有 228KB，但是其速度非常快，远高于全局内存的访问速度。  \n",
    "\n",
    "<img src=\"./images/nvidia-gpu.png\" style=\"zoom: 70%;\" />  \n",
    "\n",
    "对于常规的自注意力机制，在计算时会涉及到多次访问全局内存的情况: 首先从全局内存中读取矩阵 Q 和 K，并将计算好的矩阵 S 再写入全局内存，之后再从全局内存中获取矩阵 S，计算 Softmax 得到矩阵 P，再写入全局内容，之后读取矩阵 P 和矩阵 V ，计算得到矩阵矩阵 O。这样的过程会极大占用显存的带宽。在自注意力机制中，计算速度比内存速度快得多，因此计算效率越来越多地受到全局内存访问的瓶颈。  \n",
    "\n",
    "对FlashAttention，并没有将 S、P 整体写入全局内存，而是通过分块写入，存储前向传递的 Softmax 归一化因子，在后向传播中快速重新计算片上注意力，这比从全局内存中读取中间注意力矩阵的标准方法更快。  \n",
    "\n",
    "<img src=\"./images/flash-attention.png\" style=\"zoom: 70%;\" />  \n",
    "\n",
    "##### 3.多查询注意力\n",
    "\n",
    "多查询注意力（Multi Query Attention）是多头注意力的一种变体。其主要区别在于，在多查询注意力中不同的注意力头共享一个键和值的集合，每个头只单独保留了一份查询参数。因此键和值的矩阵仅有一份，这大幅度减少了显存占用，使其更高效。由于多查询注意力改变了注意力机制的结构，因此模型通常需要从训练开始就支持多查询注意力。\n",
    "\n",
    "多头注意力: query/key/value -> (1, 512, 768)  \n",
    "多查询注意力: query -> (1, 512, 768), key/value -> (1, 512, 96)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
