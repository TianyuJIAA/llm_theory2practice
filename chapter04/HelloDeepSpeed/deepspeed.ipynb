{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码的测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorboard demo       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "# 创建一个 SummaryWriter 实例\n",
    "writer = SummaryWriter('runs/experiment_1')\n",
    "\n",
    "# 模拟训练过程\n",
    "for epoch in range(100):\n",
    "    # 假设我们在每个 epoch 计算了一个 loss\n",
    "    loss = np.random.random()\n",
    "    \n",
    "    # 将 loss 写入 TensorBoard\n",
    "    writer.add_scalar('Loss/train', loss, epoch)\n",
    "\n",
    "# 关闭 writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### masking_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple, Union\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "TokenizerType = Union[PreTrainedTokenizer, PreTrainedTokenizerFast]\n",
    "\n",
    "def masking_function(\n",
    "        text: str,\n",
    "        tokenizer: TokenizerType,\n",
    "        mask_prob: float,\n",
    "        random_replace_prob: float,\n",
    "        unmask_replace_prob: float,\n",
    "        max_length: int,\n",
    ") -> Tuple[List[int], List[int]]:\n",
    "    tokenized_ids = ([tokenizer.bos_token_id] +\n",
    "                     tokenizer.encode(text,\n",
    "                                      add_special_tokens=False,\n",
    "                                      truncation=True,\n",
    "                                      max_length=max_length - 2) +\n",
    "                     [tokenizer.eos_token_id])\n",
    "    seq_len = len(tokenized_ids)\n",
    "    tokenized_ids = np.array(tokenized_ids)\n",
    "    subword_mask = np.full(len(tokenized_ids), False)\n",
    "\n",
    "    # Masking the BOS and EOS token leads to slightly worse performance\n",
    "    low = 1\n",
    "    high = len(subword_mask) - 1\n",
    "    mask_choices = np.arange(low, high)\n",
    "    num_subwords_to_mask = max(\n",
    "        int((mask_prob * (high - low)) + np.random.rand()), 1)\n",
    "    subword_mask[np.random.choice(mask_choices,\n",
    "                                  num_subwords_to_mask,\n",
    "                                  replace=False)] = True\n",
    "\n",
    "    # Create the labels first\n",
    "    labels = np.full(seq_len, tokenizer.pad_token_id)\n",
    "    labels[subword_mask] = tokenized_ids[subword_mask]\n",
    "\n",
    "    tokenized_ids[subword_mask] = tokenizer.mask_token_id\n",
    "\n",
    "    # Now of the masked tokens, choose how many to replace with random and how many to unmask\n",
    "    rand_or_unmask_prob = random_replace_prob + unmask_replace_prob\n",
    "    if rand_or_unmask_prob > 0:\n",
    "        rand_or_unmask = subword_mask & (np.random.rand(len(tokenized_ids)) <\n",
    "                                         rand_or_unmask_prob)\n",
    "        if random_replace_prob == 0:\n",
    "            unmask = rand_or_unmask\n",
    "            rand_mask = None\n",
    "        elif unmask_replace_prob == 0:\n",
    "            unmask = None\n",
    "            rand_mask = rand_or_unmask\n",
    "        else:\n",
    "            unmask_prob = unmask_replace_prob / rand_or_unmask_prob\n",
    "            decision = np.random.rand(len(tokenized_ids)) < unmask_prob\n",
    "            unmask = rand_or_unmask & decision\n",
    "            rand_mask = rand_or_unmask & (~decision)\n",
    "        if unmask is not None:\n",
    "            tokenized_ids[unmask] = labels[unmask]\n",
    "        if rand_mask is not None:\n",
    "            weights = np.ones(tokenizer.vocab_size)\n",
    "            weights[tokenizer.all_special_ids] = 0\n",
    "            probs = weights / weights.sum()\n",
    "            num_rand = rand_mask.sum()\n",
    "            tokenized_ids[rand_mask] = np.random.choice(tokenizer.vocab_size,\n",
    "                                                        num_rand,\n",
    "                                                        p=probs)\n",
    "    return tokenized_ids.tolist(), labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 3.73MB/s]\n",
      "Downloading: 100%|██████████| 25.0/25.0 [00:00<00:00, 205kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本:\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "掩码后的token IDs:\n",
      "[0, 133, 2119, 6219, 50264, 13855, 81, 5, 22414, 2335, 4, 2]\n",
      "\n",
      "对应的label IDs:\n",
      "[1, 1, 1, 1, 23602, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "解码后的掩码文本:\n",
      "<s>The quick brown<mask> jumps over the lazy dog.</s>\n",
      "\n",
      "被掩码的位置:\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "# 首先导入masking_function\n",
    "# 假设masking_function已经定义在当前文件中\n",
    "\n",
    "def demo_masking_function():\n",
    "    # 初始化tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "    \n",
    "    # 设置参数\n",
    "    text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    mask_prob = 0.15\n",
    "    random_replace_prob = 0.1\n",
    "    unmask_replace_prob = 0.1\n",
    "    max_length = 20\n",
    "    \n",
    "    # 调用masking_function\n",
    "    masked_tokens, labels = masking_function(\n",
    "        text=text,\n",
    "        tokenizer=tokenizer,\n",
    "        mask_prob=mask_prob,\n",
    "        random_replace_prob=random_replace_prob,\n",
    "        unmask_replace_prob=unmask_replace_prob,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    # 打印结果\n",
    "    print(\"原始文本:\")\n",
    "    print(text)\n",
    "    print(\"\\n掩码后的token IDs:\")\n",
    "    print(masked_tokens)\n",
    "    print(\"\\n对应的label IDs:\")\n",
    "    print(labels)\n",
    "    \n",
    "    # 解码masked_tokens,以便查看实际的掩码效果\n",
    "    decoded_masked = tokenizer.decode(masked_tokens)\n",
    "    print(\"\\n解码后的掩码文本:\")\n",
    "    print(decoded_masked)\n",
    "    \n",
    "    # 显示哪些位置被掩码了\n",
    "    mask_positions = [i for i, (m, l) in enumerate(zip(masked_tokens, labels)) if m != l]\n",
    "    print(\"\\n被掩码的位置:\")\n",
    "    print(mask_positions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)  # 为了结果可复现\n",
    "    demo_masking_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练报错及解决方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### error: write() argument must be str, not RunningCommand\n",
    "\n",
    "```python\n",
    "try:\n",
    "    gitlog = sh.git.log(\"-1\", format=\"%H\", _tty_out=False, _fg=False)\n",
    "    with (exp_dir / \"githash.log\").open(\"w\") as handle:\n",
    "        handle.write(str(gitlog))\n",
    "except sh.ErrorReturnCode_128:\n",
    "    logger.info(\"Seems like the code is not running from\"\n",
    "                \" within a git repo, so hash will\"\n",
    "                \" not be stored. However, it\"\n",
    "                \" is strongly advised to use\"\n",
    "                \" version control.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载Dataset出现错误\n",
    "\n",
    "应该是网络问题，出现\"ConnectionError: Couldn't reach https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\"报错\n",
    "\n",
    "解决方法:  \n",
    "目前只能从官网下载数据集，然后加载。[wikitext-2-v1](https://huggingface.co/datasets/Salesforce/wikitext/tree/main/wikitext-2-v1)  \n",
    "当然官网也有hugginface也有说明，数据是存储在git lfs上的，现将模型跑通，后面补充学习git lfs的知识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f0e3d46d0fd4a82d\n",
      "Reusing dataset parquet (/Users/jiatianyu/.cache/huggingface/datasets/parquet/default-f0e3d46d0fd4a82d/0.0.0/9296ce43568b20d72ff8ff8ecbc821a16b68e9b8b7058805ef11f06e035f911a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 36718\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 测试下读取\n",
    "from datasets import load_dataset\n",
    "\n",
    "wikitext_dataset = load_dataset(\"parquet\", \n",
    "                                data_files={\"train\": \"data/train-00000-of-00001.parquet\"},\n",
    "                                split=\"train\")\n",
    "\n",
    "print(wikitext_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### package包的问题\n",
    "\n",
    "错误信息:packaging.version.InvalidVersion: Invalid version: '0.10.1,<0.11'  \n",
    "\n",
    "解决方法:\n",
    "```shell\n",
    "pip install packaging==21.3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练效果对比\n",
    "\n",
    "设备上3090，单卡，常规使用GPU训练每3s大约10个epoch，通过deepspeed加速后可以达到每s大约10个epoch，速度提升明显，当然batchsize设置的不太合理，因为文本比较小  \n",
    "\n",
    "<img src=\"./images/训练效果.png\" alt=\"Broadcast\" style=\"zoom:60%;\"> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
