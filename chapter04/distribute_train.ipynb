{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分布式训练\n",
    "\n",
    "随着模型参数和训练数据的急速增长，单机已经无法满足模型的训练需求，因此需要设计分布式训练系统来解决海量的计算和内存资源要求问题。本节将会依次介绍机器学习系统的基础概念、分布式训练集群架构、分布式训练并行策略，并以DeepSpeed为例介绍如何在大集群上训练大语言模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分布式训练概述\n",
    "\n",
    "`分布式训练`是指将机器学习或深度学习模型训练任务分解成多个子任务，并在多个计算设备上并行地进行训练。计算设备可以是中央处理器（Central Processing Unit，CPU）、图形处理器（Graphics Processing Unit，GPU）、张量处理器（Tensor Processing Unit，TPU）也可以是神经网络处理器（Neural network Processing Unit，NPU）。  \n",
    "\n",
    "<img src=\"./images/multi_device.png\" style=\"zoom:70%;\" /> \n",
    "\n",
    "由于同一个服务器内部的多个计算设备(单机多卡)之间内存也可能并不共享，因此无论这些计算设备是否处于同一个服务器还是多个服务器中，其系统架构都属于分布式系统范畴。  \n",
    "\n",
    "单设备计算速度主要由单块计算加速芯片的`运算速度`和`数据I/O能力`来决定，对单设备训练效率进行优化，主要的技术手段有混合精度训练、算子融合、梯度累加等；分布式训练系统中计算设备数量越多，其理论峰值计算速度就会越高，但是受到通讯效率的影响，计算设备数量增大则会造成加速比急速降低；多设备加速比则是由计算和`通讯效率`决定，需要结合算法和网络拓扑结构进行优化，分布式训练并行策略主要目标就是提升分布式训练系统中的多设备加速比。  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分布式训练并行策略\n",
    "\n",
    "单节点模型训练系统主要由数据和模型两部分组成，训练过程会有多个数据小批次(mini-batch)完成。训练系统利用批次数据通过多个算子(神经网络层)根据损失函数和优化算法来生成梯度，从而调整模型参数。整个模型训练的过程是可以有一个计算图来表示:  \n",
    "\n",
    "<img src=\"./images/single_train_system.png\" style=\"zoom:70%;\" /> \n",
    "\n",
    "计算图的执行过程可以分为前向计算和反向计算两个阶段。前向计算就是将批次数据通过计算图中的每个算子，然后反向计算是根据损失函数和优化算法，每个算子计算梯度，并更新本地的参数。这样根据批次一轮一轮的进行前向传播和反向传播计算数据、更新参数。\n",
    "\n",
    "根据上面所述，很容易想到如果进行并行加速，可以从模型和数据两个维度来考虑。1.可以对数据进行切分，同时将模型复制到多个设备上，并行执行不同的数据分片，这种方式成为数据并行；2.可以对模型进行切分，将模型中的算子分发到多个设备分别完成，称为模型并行；3.可以结合上面说的两种并行方式，称为混合并行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据并行\n",
    "\n",
    "在数据并行系统中，每个计算设备都有整个模型的副本，在训练时每个计算设备都会分配一个批次数据样本的子集，进行前向传播计算。在前向计算完成后，每个计算设备需要计算所有算子的梯度，并将本地梯度进行广播。需要聚合设备的本地梯度后然后取均值对模型进行更新，完成该批次的训练。  \n",
    "\n",
    "<img src=\"./images/data_parallel.png\" style=\"zoom:70%;\" /> \n",
    "\n",
    "数据并行训练系统可以通过增加计算设备，有效提升整体训练吞吐量，即`每秒全局批次数`。与单计算设备的区别在于反向传播阶段的梯度需要在所有设备完成同步，并计算平均梯度来更新参数。TensorFlow和Pytorch都有对应的实现方法，TensorFlow DistributedStrategy、PyTorch Distributed、Horovod DistributedOptimizer。\n",
    "\n",
    "数据并行训练加速比最高，同时显存占用也比较高，主要是要求每个设备上都备份一分模型导致的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 PyTorch DistributedDataParallel 实现单个服务器多加速卡训练代码\n",
    "\n",
    "# from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "# DistributedSampler类\n",
    "\n",
    "import math\n",
    "from typing import TypeVar, Optional, Iterator\n",
    "\n",
    "import torch\n",
    "from . import Sampler, Dataset\n",
    "import torch.distributed as dist\n",
    "\n",
    "__all__ = [\"DistributedSampler\", ]\n",
    "\n",
    "T_co = TypeVar('T_co', covariant=True)\n",
    "\n",
    "\n",
    "class DistributedSampler(Sampler[T_co]):\n",
    "    r\"\"\"Sampler that restricts data loading to a subset of the dataset.\n",
    "\n",
    "    It is especially useful in conjunction with\n",
    "    :class:`torch.nn.parallel.DistributedDataParallel`. In such a case, each\n",
    "    process can pass a :class:`~torch.utils.data.DistributedSampler` instance as a\n",
    "    :class:`~torch.utils.data.DataLoader` sampler, and load a subset of the\n",
    "    original dataset that is exclusive to it.\n",
    "\n",
    "    .. note::\n",
    "        Dataset is assumed to be of constant size and that any instance of it always\n",
    "        returns the same elements in the same order.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset used for sampling.\n",
    "        num_replicas (int, optional): Number of processes participating in\n",
    "            distributed training. By default, :attr:`world_size` is retrieved from the\n",
    "            current distributed group.\n",
    "        rank (int, optional): Rank of the current process within :attr:`num_replicas`.\n",
    "            By default, :attr:`rank` is retrieved from the current distributed\n",
    "            group.\n",
    "        shuffle (bool, optional): If ``True`` (default), sampler will shuffle the\n",
    "            indices.\n",
    "        seed (int, optional): random seed used to shuffle the sampler if\n",
    "            :attr:`shuffle=True`. This number should be identical across all\n",
    "            processes in the distributed group. Default: ``0``.\n",
    "        drop_last (bool, optional): if ``True``, then the sampler will drop the\n",
    "            tail of the data to make it evenly divisible across the number of\n",
    "            replicas. If ``False``, the sampler will add extra indices to make\n",
    "            the data evenly divisible across the replicas. Default: ``False``.\n",
    "\n",
    "    .. warning::\n",
    "        In distributed mode, calling the :meth:`set_epoch` method at\n",
    "        the beginning of each epoch **before** creating the :class:`DataLoader` iterator\n",
    "        is necessary to make shuffling work properly across multiple epochs. Otherwise,\n",
    "        the same ordering will be always used.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> # xdoctest: +SKIP\n",
    "        >>> sampler = DistributedSampler(dataset) if is_distributed else None\n",
    "        >>> loader = DataLoader(dataset, shuffle=(sampler is None),\n",
    "        ...                     sampler=sampler)\n",
    "        >>> for epoch in range(start_epoch, n_epochs):\n",
    "        ...     if is_distributed:\n",
    "        ...         sampler.set_epoch(epoch)\n",
    "        ...     train(loader)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: Dataset, num_replicas: Optional[int] = None,\n",
    "                 rank: Optional[int] = None, shuffle: bool = True,\n",
    "                 seed: int = 0, drop_last: bool = False) -> None:\n",
    "        if num_replicas is None:\n",
    "            if not dist.is_available():\n",
    "                raise RuntimeError(\"Requires distributed package to be available\")\n",
    "            num_replicas = dist.get_world_size()\n",
    "        if rank is None:\n",
    "            if not dist.is_available():\n",
    "                raise RuntimeError(\"Requires distributed package to be available\")\n",
    "            rank = dist.get_rank()\n",
    "        if rank >= num_replicas or rank < 0:\n",
    "            raise ValueError(\n",
    "                \"Invalid rank {}, rank should be in the interval\"\n",
    "                \" [0, {}]\".format(rank, num_replicas - 1))\n",
    "        self.dataset = dataset\n",
    "        self.num_replicas = num_replicas\n",
    "        self.rank = rank\n",
    "        self.epoch = 0\n",
    "        self.drop_last = drop_last\n",
    "        # If the dataset length is evenly divisible by # of replicas, then there\n",
    "        # is no need to drop any data, since the dataset will be split equally.\n",
    "        if self.drop_last and len(self.dataset) % self.num_replicas != 0:  # type: ignore[arg-type]\n",
    "            # Split to nearest available length that is evenly divisible.\n",
    "            # This is to ensure each rank receives the same amount of data when\n",
    "            # using this Sampler.\n",
    "            self.num_samples = math.ceil(\n",
    "                (len(self.dataset) - self.num_replicas) / self.num_replicas  # type: ignore[arg-type]\n",
    "            )\n",
    "        else:\n",
    "            self.num_samples = math.ceil(len(self.dataset) / self.num_replicas)  # type: ignore[arg-type]\n",
    "        self.total_size = self.num_samples * self.num_replicas\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "\n",
    "    def __iter__(self) -> Iterator[T_co]:\n",
    "        if self.shuffle:\n",
    "            # deterministically shuffle based on epoch and seed\n",
    "            g = torch.Generator()\n",
    "            g.manual_seed(self.seed + self.epoch)\n",
    "            indices = torch.randperm(len(self.dataset), generator=g).tolist()  # type: ignore[arg-type]\n",
    "        else:\n",
    "            indices = list(range(len(self.dataset)))  # type: ignore[arg-type]\n",
    "\n",
    "        if not self.drop_last:\n",
    "            # add extra samples to make it evenly divisible\n",
    "            padding_size = self.total_size - len(indices)\n",
    "            if padding_size <= len(indices):\n",
    "                indices += indices[:padding_size]\n",
    "            else:\n",
    "                indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size]\n",
    "        else:\n",
    "            # remove tail of data to make it evenly divisible.\n",
    "            indices = indices[:self.total_size]\n",
    "        assert len(indices) == self.total_size\n",
    "\n",
    "        # subsample\n",
    "        indices = indices[self.rank:self.total_size:self.num_replicas]\n",
    "        assert len(indices) == self.num_samples\n",
    "\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "\n",
    "    def set_epoch(self, epoch: int) -> None:\n",
    "        r\"\"\"\n",
    "        Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas\n",
    "        use a different random ordering for each epoch. Otherwise, the next iteration of this\n",
    "        sampler will yield the same ordering.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Epoch number.\n",
    "        \"\"\"\n",
    "        self.epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 具体使用的一个demo见下面的git代码 - train_multi_gpu.py文件\n",
    "# https://github.com/TianyuJIAA/llm-course/tree/main/%E5%B0%9A%E7%A1%85%E8%B0%B7/chapter01/BertClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型并行\n",
    "\n",
    "模型并行主要用于解决单点内存不足的问题。比如GPT-3的模型参数是1750亿，模型参数都通过32位浮点数表示，则模型需要占用700G内存，计算公式为:  \n",
    "```text\n",
    "1 GB = 10^3 MB = 10^6 KB = 10^9 Bytes\n",
    "32bit = 4Bytes\n",
    "1750*10^8*4 = 175*4%10^9 = 700GB\n",
    "```\n",
    "H100加速也仅支持80GB显存，所以无法将模型完整放入其中。模型并行从计算图角度可以从两种方式切分:  \n",
    "1.按模型的层切分到不同设备，即层并行或算子间并行，也称之为流水线并行  \n",
    "2.将计算图层内的参数切分到不同设备，即层内并行或算子间并行，也称为张量并行。  \n",
    "\n",
    "<img src=\"./images/model_parallel.png\" style=\"zoom:70%;\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 流水线并行\n",
    "\n",
    "`流水线`并行是一种并行计算策略，将模型的各个层分段处理，并将每个段分布到不同的计算设备上，使得前后阶段如流水线一样工作。  \n",
    "\n",
    "典型的流水线并行系统如图所示，包含了前向和后向计算，这种并行系统中的设备长时间处于空闲状态，使用率大幅降低，形成了模型并行气泡，也称为流水线气泡。\n",
    "\n",
    "<img src=\"./images/pipeline_bubble.png\" style=\"zoom:70%;\" /> \n",
    "\n",
    "针对朴素流水线并行系统使用率低的问题，提出GPipe方法。将小批次进一步划分为更小的微批次，利用流水线并行方案，每次处理一个微批次的数据。通过这种方式有效地降低了并行气泡，但是只有当一个Mini-batch中所有的前向计算完成才能够开始执行后向计算，也降低了系统的并行效率。  \n",
    "\n",
    "<img src=\"./images/Gpipe.png\" style=\"zoom:70%;\" /> \n",
    "\n",
    "1F1B流水线策略引入了任务调度机制，使下游设备在等待上游计算的同时执行其他可并行的任务，提高设备的利用率。  \n",
    "\n",
    "<img src=\"./images/1F1B.png\" style=\"zoom:70%;\" /> \n",
    "\n",
    "Pytorch中包含了实现流水线的API函数Pipe，具体参考: torch.distributed.pipeline.sync.Pipe  \n",
    "\n",
    "```python\n",
    "# Step 0. Need to initialize RPC framework first.\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '29500'\n",
    "torch.distributed.rpc.init_rpc('worker', rank=0, world_size=1)\n",
    "\n",
    "# Step 1: build a model including two linear layers\n",
    "fc1 = nn.Linear(16, 8).cuda(0)\n",
    "fc2 = nn.Linear(8, 4).cuda(1)\n",
    "\n",
    "# Step 2: wrap the two layers with nn.Sequential\n",
    "model = nn.Sequential(fc1, fc2)\n",
    "\n",
    "# Step 3: build Pipe (torch.distributed.pipeline.sync.Pipe)\n",
    "model = Pipe(model, chunks=8)\n",
    "\n",
    "# do training/inference\n",
    "input = torch.rand(16, 16).cuda(0)\n",
    "output_rref = model(input) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 张量并行\n",
    "\n",
    "`张量并行`需要根据模型的具体结构和算子类型，解决如何将参数切分到不同设备，以及如何保证切分后数学一致性问题。其实本质上就是将矩阵按行或者按列进行切分到不同的设备中，然后分别进行矩阵的相乘，最后通过通信得到完整的全量结果。  \n",
    "\n",
    "具体以Transformer结构为基础，其主要由以下三种算子组成: Embedding、矩阵乘和交叉熵损失计算构成。  \n",
    "\n",
    "1.对于嵌入式表示算子，可以将词表进行按照词维度进行切分，每个设备只存储部分词向量，然后通过汇总各个设备的部分词向量，得到完整的词向量  \n",
    "\n",
    "<img src=\"./images/tensor_embedding.png\" style=\"zoom:70%;\" /> \n",
    "\n",
    "2.矩阵乘的张量并行要充分利用矩阵分块乘法原理，也就是按照行或者是按照列进行切分。\n",
    "\n",
    "<img src=\"./images/tensor_multi.png\" style=\"zoom:70%;\" /> \n",
    "\n",
    "将矩阵`A`按照行或者列切分为A1和A2，并放到不同的设备上，然后分别将矩阵X与其相乘，最后多个计算设备之间进行通信，并拼接在一起得到最终的结果矩阵Y  \n",
    "\n",
    "3.分类层会通过softmax和交叉熵来计算损失。如果类别数量特别大，也会导致设备内存无法存储和计算logi矩阵。所以可以按照类别维度切分，同时通过中间结果通信，得到最终的全局的交叉熵损失\n",
    "\n",
    "最后附上多头注意力机制张量并行示意图:  \n",
    "\n",
    "<img src=\"./images/tensor_mlh.png\" style=\"zoom:70%;\" /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch提供了细粒度张量级别的并行API，DistributedTensor，也提供了粗粒度模型层面的API可以对\"nn.Module\"进行张量并行。\n",
    "# 这个from torch.distributed._tensor引用有问题先跳过\n",
    "import torch\n",
    "from torch.distributed._tensor import DTensor, DeviceMesh, Shard, distribute_tensor # type: ignore\n",
    "\n",
    "# construct a device mesh with available devices (multi-host or single host)\n",
    "device_mesh = DeviceMesh(\"cuda\", [0, 1, 2, 3])\n",
    "\n",
    "# if we want to do row-wise sharding\n",
    "rowwise_placement=[Shard(0)]\n",
    "\n",
    "# if we want to do col-wise sharding\n",
    "colwise_placement=[Shard(1)]\n",
    "big_tensor = torch.randn(888, 12)\n",
    "\n",
    "# distributed tensor returned will be sharded across the dimension specified in placements\n",
    "rowwise_tensor = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=rowwise_placement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 混合并行\n",
    "\n",
    "混合并行是将多种并行策略如数据并行、流水线并行和张量并行等进行混合使用。通过结合不同的并行策略，混合并行可以发挥各种并行策略的优点，最大程度地提高计算性能和效率。具体来说:  \n",
    "1.通常在每个`服务器内部`使用`张量并行`策略，由于该策略涉及的网络通信量较大，需要利用服务器内部的不同计算设备之间进行高速通信带宽。  \n",
    "2.通过`流水线并行`，将模型的不同层划分为多个阶段，每个阶段由`不同的机器`负责计算。这样可以充分利用多台机器的计算能力，并通过机器之间的高速通信来传递计算结果和中间数据，以提高整体的计算速度和效率。  \n",
    "3.最后，在外层叠加数据并行策略，以增加并发数量，提升整体训练速度。通过`数据并行`，将训练数据分发到多组服务器上进行并行处理， `每组服务器`处理不同的数据批次。这样可以充分利用多台服务器的计算资源，并增加训练的并发度，从而加快整体训练速度。  \n",
    "\n",
    "`BLOOM`是支持最多59种语言和176B参数的大语言模型模型，其训练的架构如图所示，其使用了由48个英伟达 DGX-A100服务器组成的集群，GPU数=8*48共计384张，BLOOM训练采用的策略是首先将集群分为48个一组，进行数据并行。接下来，模型整体被分为12个阶段，进行流水线并行。每个阶段的模型被划分到4张GPU中进行张量并行。  \n",
    "\n",
    "<img src=\"./images/BLOOM.png\" style=\"zoom:70%;\" /> \n",
    "\n",
    "NVIDIA DGX A100是一款极其强大的AI超级计算机。它装配了8块A100 GPU，能够提供高达5 PetaFLOPS的AI性能，从而可以处理最复杂和计算密集型的AI任务。DGX A100支持英伟达的多实例GPU（MIG）技术，可以将一个GPU分割为多个独立的GPU实例，这使得用户可以根据自己的需求，灵活地调整每个GPU实例的数量和大小，进一步提升硬件的利用率。此外，DGX A100还配备了全套的英伟达AI软件框架和库，如TensorFlow、PyTorch等，让用户可以立即开始AI的使用和开发，无需自己搭建软硬件环境  \n",
    "\n",
    "<img src=\"./images/DGX-A100.png\" style=\"zoom:30%;\" />  \n",
    "\n",
    "DGX A100参数  \n",
    "\n",
    "<img src=\"./images/DGX-config.png\" style=\"zoom:30%;\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 计算设备内存优化\n",
    "\n",
    "目前大模型训练通常采用Adam优化算法，其对计算设备内存的占用很大。为了降低内存占用，多数系统采用混合精度训练方式，即同时存在`FP16`或者`BF16`和`FP32`两种格式的数值。FP16的值区间比FP32要小很多，所以在计算过程中很容易出现上溢出和下溢出。BF16相比于FP16是以精度换更大的值区间范围，但是由于FP16和BF16的精度较低，训练过程中可能会出现梯度消失和模型不稳定的问题，因此需要动态损失缩放和混合精度优化器来解决这些问题。  \n",
    "\n",
    "<img src=\"./images/FP16.png\" style=\"zoom:60%;\" /> \n",
    "\n",
    "混合精度优化过程也就是优化器及模型相关参数的精度是不一样的，并涉及到精度的转换。同时在训练过程中模型状态实际上需要的内存很大，可能是100G，因此也需要减少模型状态来解决内存占用问题。  \n",
    "\n",
    "<img src=\"./images/混合精度.png\" style=\"zoom:60%;\" /> \n",
    "\n",
    "零冗余优化器(ZeRo)目标是针对模型状态的存储去除冗余的优化。ZeRO使用分区的方法，将模型状态量分割成多个分区，每个计算设备只保存其中的一部分。这个整个训练系统内只需要维护一份模型状态即可，减少了内存消耗和通信开销。  \n",
    "\n",
    "<img src=\"./images/ZeRO.png\" style=\"zoom:60%;\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch中也实现了ZeRO优化方法\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributed.optim import ZeroRedundancyOptimizer\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def print_peak_memory(prefix, device):\n",
    "    if device == 0:\n",
    "        print(f\"{prefix}: {torch.cuda.max_memory_allocated(device) // 1e6}MB \")\n",
    "\n",
    "def example(rank, world_size, use_zero):\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    # create default process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "    # create local model\n",
    "    model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])\n",
    "    print_peak_memory(\"Max memory allocated after creating local model\", rank)\n",
    "    # construct DDP model\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    print_peak_memory(\"Max memory allocated after creating DDP\", rank)\n",
    "    # define loss function and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    if use_zero:\n",
    "        optimizer = ZeroRedundancyOptimizer( # 这里使用了 ZeroRedundancyOptimizer\n",
    "            ddp_model.parameters(),\n",
    "            optimizer_class=torch.optim.Adam, # 包装了 Adam\n",
    "            lr=0.01\n",
    "            )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(ddp_model.parameters(), lr=0.01)\n",
    "    \n",
    "    # forward pass\n",
    "    outputs = ddp_model(torch.randn(20, 2000).to(rank))\n",
    "    labels = torch.randn(20, 2000).to(rank)\n",
    "    # backward pass\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    # update parameters\n",
    "    print_peak_memory(\"Max memory allocated before optimizer step()\", rank)\n",
    "    optimizer.step()\n",
    "    print_peak_memory(\"Max memory allocated after optimizer step()\", rank)\n",
    "    print(f\"params sum is: {sum(model.parameters()).sum()}\")\n",
    "\n",
    "def main():\n",
    "    world_size = 2\n",
    "    print(\"=== Using ZeroRedundancyOptimizer ===\")\n",
    "    mp.spawn(example,\n",
    "        args=(world_size, True),\n",
    "        nprocs=world_size,\n",
    "        join=True)\n",
    "    \n",
    "    print(\"=== Not Using ZeroRedundancyOptimizer ===\")\n",
    "    \n",
    "    mp.spawn(example,\n",
    "        args=(world_size, False),\n",
    "        nprocs=world_size,\n",
    "        join=True)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分布式训练的集群架构\n",
    "\n",
    "分布式训练需要使用多台服务器组成的计算集群完成。而集群的架构也需要根据分布式系统、大语言模型结构、优化算法等综合因素进行设计。分布式训练集群属于高性能计算集群，其目标是提供海量的计算能力，主要有两种常见架构：参数服务器架构和去中心化架构。  \n",
    "\n",
    "#### 高性能计算集群硬件组成\n",
    "\n",
    "高性能计算集群包含大量带有计算加速设备的服务器。每个服务器内部由多个计算加速设备(2-16)，多个服务器会被放置在一个机柜中，服务器通过架顶交换机连接网络。在架顶交换机满载的情况下，可以通过增加骨干交换机进一步接入新的机柜。这种连接服务器的拓扑结构往往是一个多层树。\n",
    "\n",
    "<img src=\"./images/multi_level_tree.png\" style=\"zoom:60%;\" />\n",
    "\n",
    "多层树结构集群中跨机柜通信往往会有网络瓶颈；单个服务器内部的计算加速设备之间的通讯带宽也是影响分布式训练的重要因素，因此服务器内部通常采用异构网络架构。NVIDIA HGX H100 8GPU服务器，采用了`NVLink`和`NVSwitch`(NVLink交换机)技术，每个H100 GPU都有多个NVLink端口，并连接到所有四个NVSwitch上。每个NVSwitch都是一个完全无阻塞的交换机，完全连接所有8个H100计算加速卡。NVSwitch的这种完全连接的拓扑结构，使得服务器内任何H100加速卡之间都可以达到 `900GB/s`双向通信速度。\n",
    "\n",
    "<img src=\"./images/HGX.png\" style=\"zoom:70%;\" />\n",
    "\n",
    "#### 参数服务器架构\n",
    "\n",
    "参数服务器架构的分布式训练中有两种服务器角色: 训练服务器和参数服务器。参数服务器需要提供充足内存资源和通信资源，训练服务器需要提供大量的计算资源。在实际训练中，训练服务器将对应分区的参数推送到参数服务器中，参数服务器会等待两个训练服务器都完成梯度推送(同步、异步)，然后开始计算平均梯度，并更新参数。  \n",
    "\n",
    "<img src=\"./images/参数服务器.png\" style=\"zoom:70%;\" />\n",
    "\n",
    "参数服务器架构分布式训练过程可以细分为同步训练和异步训练两种模式: \n",
    "\n",
    "- 同步训练: 训练服务器在完成一个小批次的训练后，将梯度推送给参数服务器。参数服务器在接收到所有训练服务器的梯度后，进行梯度聚合和参数更新。\n",
    "- 异步训练：训练服务器在完成一个小批次的训练后，将梯度推送给参数服务器。但是参数服务器不再等待接收所有训练服务器的梯度，而是直接基于已接收到的梯度进行参数更新。\n",
    "\n",
    "#### 去中心化架构\n",
    "\n",
    "去中心化架构采用集合通信实现分布式训练系统。在去中心化架构中，没有中央服务器或控制节点，而是由节点之间进行直接通信和协调。好处是可以减少通信瓶颈、降低通信开销，提高系统的可扩展性。在分布式训练中，节点之间需要周期性地交换参数更新和梯度信息。可以通过集合通信技术来实现，下面主要介绍下常见的通信原语。  \n",
    "\n",
    "- `BroadCast`: 主节点把自身的数据发送到集群中的其他节点。分布式训练系统中常用于网络参数的初始化。计算设备1将大小为1 × N的张量进行广播，最终每张卡输出均为[1 × N]的矩阵\n",
    "\n",
    "<div style=\"text-align:center;\">  \n",
    "    <img src=\"./images/broadcast.png\" alt=\"Broadcast\" style=\"zoom:70%;\">  \n",
    "</div>\n",
    "\n",
    "- `Scatter`: 与broadcast很类似，scatter是将数据进行划分并散布只其他指定的节点\n",
    "\n",
    "<div style=\"text-align:center;\">  \n",
    "    <img src=\"./images/scatter.png\" alt=\"Broadcast\" style=\"zoom:70%;\">  \n",
    "</div>\n",
    "\n",
    "- `Reduce`: 是一系列简单运算操作的统称，将不同节点上的计算结果进行聚合(Aggregation)\n",
    "\n",
    "<div style=\"text-align:center;\">  \n",
    "    <img src=\"./images/reduce.png\" alt=\"Broadcast\" style=\"zoom:70%;\">  \n",
    "</div>\n",
    "\n",
    "- `All Reduce`: 在所有的节点上都应用同样的Reduce操作，可通过单节点上Reduce+Broadcast操作完成\n",
    "\n",
    "<div style=\"text-align:center;\">  \n",
    "    <img src=\"./images/all_reduce.png\" alt=\"Broadcast\" style=\"zoom:70%;\">  \n",
    "</div>\n",
    "\n",
    "- `Gather`: 将多个节点上的数据收集到单个节点上，Gather可以理解为方向的Scatter\n",
    "\n",
    "<div style=\"text-align:center;\">  \n",
    "    <img src=\"./images/gather.png\" alt=\"Broadcast\" style=\"zoom:70%;\">  \n",
    "</div>\n",
    "\n",
    "- `All Gather`: 收集其他所有节点的数据到节点上，相当于Gather+Broadcast\n",
    "\n",
    "<div style=\"text-align:center;\">  \n",
    "    <img src=\"./images/all_gather.png\" alt=\"Broadcast\" style=\"zoom:70%;\">  \n",
    "</div>\n",
    "\n",
    "- `Reduce Scatter`: 将每个节点中的张量切分为多个块，每个块分配给不同的节点。接收到的块会在每个节点上进行特别的操作\n",
    "\n",
    "<div style=\"text-align:center;\">  \n",
    "    <img src=\"./images/reduce_scatter.png\" alt=\"Broadcast\" style=\"zoom:70%;\">  \n",
    "</div>\n",
    "\n",
    "- `All to All`: 将每个节点的张量切分为多个块，每个块分别发送给不同的节点\n",
    "\n",
    "<div style=\"text-align:center;\">  \n",
    "    <img src=\"./images/all2all.png\" alt=\"Broadcast\" style=\"zoom:70%;\">  \n",
    "</div>\n",
    "\n",
    "分布式集群中网络硬件多种多样，包括以太网、InfiniBand 网络等。Pytorch 等深度学习框架通常不直接操作硬件，而是使用通信库。常用的通信库包括MPI、GLOO和NCCL等，可以根据具体情况进行选择和配置  \n",
    "\n",
    "<div style=\"text-align:center;\">  \n",
    "    <img src=\"./images/nccl.png\" alt=\"Broadcast\" style=\"zoom:70%;\">  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSpeed实践\n",
    "\n",
    "DeepSpeed是一个由Microsoft公司开发的开源深度学习优化库，旨在提高大规模模型训练的效率和可扩展性，使研究人员和工程师能够更快地迭代和探索新的深度学习模型和算法。它采用了多种技术手段来加速训练，包括模型并行化、梯度累积、动态精度缩放和本地模式混合精度等。此外，DeepSpeed还提供了一些辅助工具，例如分布式训练管理、内存优化和模型压缩，以帮助开发者更好地管理和优化大规模深度学习训练任务。  \n",
    "\n",
    "DeepSpeed主要优势在于支持大规模神经网络模型、提供了更多的优化策略和工具。DeepSpeed通过实现三种并行方法的灵活组合，即ZeRO支持的数据并行、流水线并行和张量并行，可以应对不同工作负载的需求。特别是通过 3D 并行性的支持，DeepSpeed可以处理具有万亿参数的超大规模模型。DeepSpeed还引入了 ZeRO-Offload，使单个 GPU 能够训练比其显存大小大 10 倍的模型。\n",
    "\n",
    "DeepSpeed3D并行策略如图所示, 图中给出了包含32个计算设备进行3D并行的例子。神经网络的各层分为4个流水线阶段。每个流水线阶段中的层在4个张量并行计算设备之间进一步划分。最后，每个流水线阶段有两个数据并行实例，使用ZeRO内存优化在这2个副本之间划分优化器状态量  \n",
    "\n",
    "<div style=\"text-align:center;\">  \n",
    "    <img src=\"./images/3d_parallel.png\" alt=\"Broadcast\" style=\"zoom:60%;\">  \n",
    "</div>\n",
    "\n",
    "DeepSpeed软件架构主要包含三部分:  \n",
    "- APIs：DeepSpeed 提供了易于使用的API接口，简化了训练模型和推断的过程。用户只需通过调用几个API接口即可完成任务。通过initialize接口可以初始化引擎，并在参数中配置训练参数和优化技术等。这些配置参数通常保存在名为\"ds_config.json\"的文件中。\n",
    "- RunTime：DeepSpeed的核心运行时组件，使用Python语言实现，负责管理、执行和优化性能。它承担了将训练任务部署到分布式设备的功能，包括数据分区、模型分区、系统优化、微调、故障检测以及检查点的保存和加载等任务。\n",
    "-  Ops：DeepSpeed 的底层内核组件，使用C++和CUDA实现。它优化计算和通信过程，提供了一系列底层操作，包括Ultrafast Transformer Kernels、fuse LAN kernels、Customary Deals等。Ops的目标是通过高效的计算和通信加速深度学习训练过程。\n",
    "\n",
    "<div style=\"text-align:center;\">  \n",
    "    <img src=\"./images/ds软件架构.png\" alt=\"Broadcast\" style=\"zoom:60%;\">  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基础概念\n",
    "\n",
    "DeepSpeed提供了分布式计算框架，先明确几个重要的基础概念: 节点编号、全局进程编号、局部进程编号、全局总进程数和主节点。\n",
    "- 主节点: DeepSpeed主节点(master_ip+master_port)负责协调所有其他节点和进程的工作，由主节点所在服务器的IP地址和主节点进程的端口号来确定主节点。主节点还负责监控系统状态、处理任务分配和结果汇总等任务，是整个系统的关键\n",
    "- 节点编号(node_rank): 是系统中每个节点的唯一标识符，用于区分不同进程之间的通信\n",
    "- 全局进程编号(rank): 是整个系统中的每个进程的唯一标识符，用于区分不同进程之间的通信\n",
    "- 局部进程编号(local_rank): 是单个节点内的每个进程的唯一标识符，用于区分同一节点的不同进程之间的通信\n",
    "- 全局总进程数(word_size): 是整个系统中运行的所有进程的总数，用于确定可以并行完成多少工作以及需要完成任务所需的资源数量\n",
    "  \n",
    "网络通信策略上，DeepSpeed提供了MPI、GLOO和NCCL等选项，可以视情况进行选择和配置，比如:\n",
    "```json\n",
    "{\n",
    "    \"optimizer\": {\n",
    "    \"type\": \"OneBitAdam\",\n",
    "    \"params\": {\n",
    "            \"lr\": 0.001,\n",
    "            \"betas\": [\n",
    "            0.8,\n",
    "            0.999\n",
    "        ],\n",
    "        \"eps\": 1e-8,\n",
    "        \"weight_decay\": 3e-7,\n",
    "        \"freeze_step\": 400,\n",
    "        \"cuda_aware\": false,\n",
    "        \"comm_backend_name\": \"nccl\"\n",
    "        } \n",
    "    }\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "DeepSpeed也支持多种类型Zero的分片机制，比如使用Zero-3配置参数样例:  \n",
    "```json\n",
    "{\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "    },\n",
    "    \"fp16\": {\n",
    "        \"enabled\": true\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": 0.001,\n",
    "            \"betas\": [\n",
    "                0.8,\n",
    "                0.999\n",
    "            ],\n",
    "            \"eps\": 1e-8,\n",
    "            \"weight_decay\": 3e-7\n",
    "        }\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "还有将优化器状态、计算、模型参数装载到CPU内存的操作。。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实践部分\n",
    "\n",
    "以微软在github上提供的[DeepSpeed example](https://github.com/microsoft/DeepSpeedExamples)为例，看下是如何加速模型的，主要是training目录下的HelloDeepSpeed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
