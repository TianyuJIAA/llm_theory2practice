{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对google提供的Deepspeed-Chat代码进行学习，同时在AUTO-DL平台进行SFT  \n",
    "目前打算分别通过DeepSpeed-Chat框架训练英文和中文模型各一个，需要选择较小的模型+数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSpeed-Chat\n",
    "\n",
    "DeepSpeed-Chat是微软于2023年发布的基于Deepspeed用于训练类ChatGPT模型的开发工具，其具有以下三大核心功能:  \n",
    "\n",
    "- 易用的类ChatGPT模型的训练和强化推理：只需要一个脚本就可以实现多个训练步骤，包括使用Huggingface预训练的模型，使用InstructGPT训练的所有三个步骤构建类ChatGPT模型。此外，还提供了一个易于使用的推理API，用于用户在模型训练后对话式交互性测试。\n",
    "- DeepSpeed-RLHF管道：DeepSpeed-RLHF复现了InstructGPT论文中的训练模式，包括监督微调、奖励模型微调以及基于人类反馈的强化学习三个步骤。此外，还提供了数据抽象和混合功能，以支持用户使用多个不同来源的数据源进行训练。\n",
    "- DeepSpeed-RLHF系统：将DeepSpeed的训练（Training Engine）和推理能力（Inference Engine)整合到统一的混合引擎（DeepSpeed Hybrid Engine，DeepSpeed-HE）中用于RLHF训练。DeepSpeed-HE能够无缝地在推理和训练模式之间切换，使其能够利用来自DeepSpeed-Inference的各种优化。\n",
    "\n",
    "复现InstructGPT三个步骤:  \n",
    "\n",
    "<img src=\"assets/image/ppo_trainer.png\" style=\"zoom:40%;\" alt=\"DeepSpeed RLHF ppo trainer!\"/>\n",
    "\n",
    "具体来说:  \n",
    "\n",
    "1.监督微调(SFT): 使用精选的人类回答类微调预训练的语言模型以应对各种查询  \n",
    "2.奖励模型微调: 使用一个包含人类对同一个查询的多个答案打分的数据集来训练一个独立的奖励模型(RW)  \n",
    "3.RLHF训练: 利用PPO算法，根据RW模型的奖励反馈进一步微调SFT模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto DL\n",
    "\n",
    "### 租卡\n",
    "\n",
    "先租一块3090跑下三个步骤，不够的话再租一块，硬盘也需要扩一下，扩容100G  \n",
    "\n",
    "看下具体配置:  \n",
    "\n",
    "<img src=\"assets/image/autodl.png\" style=\"zoom:80%;\"/>\n",
    "\n",
    "### 容器基础配置\n",
    "\n",
    "> \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
