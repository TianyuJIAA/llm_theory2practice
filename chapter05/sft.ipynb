{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有监督微调\n",
    "\n",
    "上一章介绍了分布式训练基本概念、分布式策略，以及Deepspeed是如何加速模型的训练。本章将介绍有监督微调（Supervised Fine-Tuning, SFT），也就是在已经训练好的模型基础上，使用有标注的特定任务数据进行下一步的微调，从而使模型具备遵循指令的能力。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提示学习和语境学习\n",
    "\n",
    "#### 提示学习\n",
    "\n",
    "提示学习不同于传统的监督学习，直接利用在大量原始文本上进行预训练的语言模型，通过定义提示函数，使模型可以执行小样本甚至零样本学习。  \n",
    "\n",
    "提示学习流程非常简洁，可以描述为三个阶段：提示添加、答案搜索和答案映射:  \n",
    "\n",
    "<img src=\"./images/提示学习.png\" style=\"zoom:60%;\" /> \n",
    "\n",
    "具体来说，原始输入`x`经过一个模板，被构造成一个提示，然后将其输入语言模型，语言模型即以概率的方式填充模版中待填充的内容，然后根据模型的输出即可得到最终的预测标签。\n",
    "\n",
    "#### 语境学习\n",
    "\n",
    "语境学习也称上下文学习，是指模型可以从上下文的几个例子中学习：向模型输入特定任务的一些具体例子以及要测试的样例，模型可以根据给定的实力续写出测试样例的答案。语境学习可以看作是提示学习的一个子类。语境学习的关键思想是从类比中学习，整个过程并不需要对模型进行参数更新，仅执行向前的推理。  \n",
    "\n",
    "<img src=\"./images/语境学习.png\" style=\"zoom:60%;\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高效模型微调\n",
    "\n",
    "由于大模型的参数量十分庞大，当其应用到下游任务时，微调全部参数需要相当高的算力，因此为了节省成本，研究人员提出了多种参高效的微调方法，旨在仅训练少量参数使模型适应到下游任务。接下来主要是介绍LoRA，LoRA方法可以在缩减训练参数量和GPU显存占用的同时，使训练后的模型具有与全量微调相当的性能。  \n",
    "\n",
    "#### LoRA\n",
    "\n",
    "研究人员认为参数更新量即使投影到较小的子空间中，也不会影响到学习的有效性。因此，提出固定预训练模型参数不变，在原本权重矩阵旁路添加低秩矩阵的乘积作为可训练参数，用以模拟参数的变化量，LoRA方法的计算流程图为:  \n",
    "\n",
    "<img src=\"./images/LoRA结构图.png\" style=\"zoom:60%;\" /> \n",
    "\n",
    "具体来说，假设预训练权重为$W_0$(d * k)，可以通过可训练参数$\\Delta W = BA $变为(d * r + r * d)，大大的减少了训练参数的子空间，公式为:  \n",
    "\n",
    "$$\n",
    "h = W_0 x + \\Delta W x = W_0 x + B A x\n",
    "$$\n",
    "\n",
    "对于使用LoRA的模型来说，由于可以将原始权重与训练后权重合并，即$W = W_0 + B A$，因此在推理时不存在额外的开销。\n",
    "\n",
    "peft库中含有LoRA在内的多种高效微调方法，且与transformer兼容示例为: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "model_name_or_path = \"bigscience/mt0-large\"\n",
    "tokenizer_name_or_path = \"bigscience/mt0-large\"\n",
    "\n",
    "'''\n",
    "在Lora的配置项中有两个参数: r和lora_alpha\n",
    "r: Lora attention dimension, r即rank 是矩阵的秩(rank), 用于控制LoRA中低秩矩阵的维度\n",
    "lora_alpha: 是一个放缩系数，用于控制微调过程中更新矩阵 ΔW 的大小, 也就是控制其对原始模型的影响\n",
    "'''\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# get_peft_model函数包裹了基础模型得到一个PeftModel类的模型，如果使用lora微调方法则会得到LoraModel类的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LoRA的变体\n",
    "\n",
    "1.AdaLoRA  \n",
    "\n",
    "Lora算法给所有的低秩矩阵指定了唯一的秩，从而忽略了不同模块、不同层的参数对于微调特定任务的重要性差异。因此可以在微调过程中根据各权重矩阵对下游任务的重要性动态调整秩的大小，用以进一步减少可训练参数量的同时保持或提高性能。  \n",
    "\n",
    "2.QLoRA  \n",
    "\n",
    "QLoRA并没有对LoRA的逻辑作出修改，而是通过将预训练模型量化为4-bit以进一步节省计算开销。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LoRA demo\n",
    "\n",
    "通过ChatGPT生成的使用了Lora和未使用Lora的模型训练过程作个对比。  \n",
    "数据量比较小，但是使用lora的模型还是要比不使用的开了一倍的时间，参数越多越明显"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# mac GPU加速\n",
    "import torch\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# install packages\n",
    "pip install torch transformers datasets peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未使用Lora\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 使用 distilbert 基础模型\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 加载 imdb 数据集 本地加载数据集一直出错，所以在Google Colab中试下\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Split the dataset into train and eval sets\n",
    "# Assuming you want to use 1000 samples for training and 500 for evaluation\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select([i for i in range(1000)])\n",
    "eval_dataset = dataset[\"test\"].shuffle(seed=42).select([i for i in range(500)]) \n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "encoded_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "encoded_eval_dataset = eval_dataset.map(preprocess_function, batched=True) # Preprocess the eval dataset\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_no_lora',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# 创建trainer, providing both train_dataset and eval_dataset\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_eval_dataset, # Pass the evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 记录训练开始时间\n",
    "start_time = time.time()\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "\n",
    "# 记录训练结束时间\n",
    "end_time = time.time()\n",
    "\n",
    "# 打印训练时长\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training completed in: {training_time // 60:.0f} minutes and {training_time % 60:.0f} seconds\")\n",
    "\n",
    "model.save_pretrained(\"./distilbert_no_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# 使用 distilbert 基础模型\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# LoRA 配置\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # 序列分类任务\n",
    "    inference_mode=False,\n",
    "    r=8,  # 秩\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_lin', 'k_lin']  # 指定目标模块\n",
    ")\n",
    "\n",
    "# 应用 LoRA 配置\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 加载 imdb 数据集\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Split the dataset into train and eval sets\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select([i for i in range(1000)])\n",
    "eval_dataset = dataset[\"test\"].shuffle(seed=42).select([i for i in range(500)]) \n",
    "\n",
    "# 预处理函数\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "encoded_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "encoded_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_with_lora',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# 创建 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 记录训练开始时间\n",
    "start_time = time.time()\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "\n",
    "# 记录训练结束时间\n",
    "end_time = time.time()\n",
    "\n",
    "# 打印训练时长\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training completed in: {training_time // 60:.0f} minutes and {training_time % 60:.0f} seconds\")\n",
    "\n",
    "# 保存模型\n",
    "model.save_pretrained(\"./distilbert_with_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型上下文窗口扩展\n",
    "\n",
    "在大模型中更多长文本建模需求出现，这些任务需要模型更好地处理超出常规上下文窗口大小的文本内容。当涉及长时间对话或摘要长文档时，传统的上下文窗口大小可能无法捕捉到全局语境，从而导致信息丢失或模糊的建模结果。  \n",
    "\n",
    "为了更好地满足长文本需求，主要有以下方法来扩展语言模型的长文本建模能力:  \n",
    "- `增加上下文窗口的微调`: 采用直接的方式，即通过使用一个更长的上下文窗口来微调现有的预训练Transformer，以适应长文本建模需求\n",
    "- `位置编码`: 改进的位置编码，能够实现一定程度上的长度外推。这意味着可以在短的上下文窗口上进行训练，在长的上下文窗口中进行推理\n",
    "- `插值法`: 将超出上下文窗口的位置编码通过插值法压缩到预训练的上下文窗口中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指令数据构建\n",
    "\n",
    "#### 手动构建指令\n",
    "\n",
    "手动构建指令的方法比较直观，可以在网上收集大量的问答数据再人为加以筛选过滤，或者使用标注人员直接手动编写提示与相应的回答。以LIMA为例，其从多个来源采样收集指令数据，包括高质量网络问题社区及大量的标注人员手动编写的提示与回答。当然针对这些数据还要做进一步的处理。\n",
    "\n",
    "\n",
    "#### 自动构建指令\n",
    "\n",
    "手动构建指令数据代价高昂，需要大量的人力投入。因此，需要寻找更高效的替代方法。比如Self-Instruct，利用大模型的生成能力自动生成指令，其数据生成过程是一个迭代引导算法，包含四个步骤:  \n",
    "\n",
    "<img src=\"./images/Self-Instruct.png\" style=\"zoom:60%;\" /> \n",
    "\n",
    "#### 开源指令数据集\n",
    "\n",
    "一揽子开源指令数据集: \n",
    "\n",
    "<img src=\"./images/dataset.png\" style=\"zoom:60%;\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deepspeed-Chat SFT实践\n",
    "\n",
    "一些方法的测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SFT\n",
    "\n",
    "在对模型sft之前先测试下模型，在google colab平台测试opt-1.3b的输出  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./DeepSpeed-Chat/download/facebook/opt-350m/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./DeepSpeed-Chat/download/facebook/opt-350m/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# get_accelerator\n",
    "\n",
    "from deepspeed import get_accelerator\n",
    "\n",
    "accelerator = get_accelerator()\n",
    "device_name = accelerator.device_name()\n",
    "print(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 加载tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_tokenizer(model_name_or_path, fast_tokenizer=True):\n",
    "    print(model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path, fast_tokenizer=fast_tokenizer)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # make sure tokenizer is right pad in our logic\n",
    "    tokenizer.padding_side = 'right'\n",
    "    return tokenizer\n",
    "\n",
    "def load_hf_tokenizer(model_name_or_path,\n",
    "                      fast_tokenizer=True,\n",
    "                      add_special_tokens=None):\n",
    "    if os.path.exists(model_name_or_path):\n",
    "        # Locally tokenizer loading has some issue, so we need to force download\n",
    "        model_json = os.path.join(model_name_or_path, \"config.json\")\n",
    "        if os.path.exists(model_json):\n",
    "            model_json_file = json.load(open(model_json))\n",
    "            model_name = model_json_file.get(\"_name_or_path\",\n",
    "                                             model_name_or_path)\n",
    "            # 这个位置需要传入本地目录，而不是传入model_name\n",
    "            tokenizer = get_tokenizer(model_name_or_path,\n",
    "                                      fast_tokenizer=fast_tokenizer)\n",
    "    else:\n",
    "        tokenizer = get_tokenizer(model_name_or_path,\n",
    "                                  fast_tokenizer=fast_tokenizer)\n",
    "\n",
    "    if add_special_tokens is not None:\n",
    "        add_special_tokens = [add_special_tokens] if isinstance(add_special_tokens, str) \\\n",
    "            else add_special_tokens\n",
    "        tokenizer.add_special_tokens(\n",
    "            {'additional_special_tokens': add_special_tokens})\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./DeepSpeed-Chat/download/facebook/opt-350m/\n",
      "GPT2TokenizerFast(name_or_path='./DeepSpeed-Chat/download/facebook/opt-350m/', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model_path = './DeepSpeed-Chat/download/facebook/opt-350m/'\n",
    "tokenizer = load_hf_tokenizer(model_path)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lora的具体实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
      "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
      "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./DeepSpeed-Chat/download/facebook/opt-350m/\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.functional as F\n",
    "\n",
    "class LinearLayer_LoRA(nn.Module):\n",
    "    # an simple implementation of LoRA\n",
    "    # for now only support Linear Layer\n",
    "    def __init__(self,\n",
    "                 weight,\n",
    "                 lora_dim=0,\n",
    "                 lora_scaling=1,\n",
    "                 lora_droppout=0,\n",
    "                 bias=None):\n",
    "        super(LinearLayer_LoRA, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "\n",
    "        if lora_dim <= 0:\n",
    "            raise ValueError(\n",
    "                \"You are training to use LoRA, whose reduced dim should be larger than 1\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            # for zero stage 3\n",
    "            rows, columns = weight.ds_shape\n",
    "        except:\n",
    "            rows, columns = weight.shape\n",
    "        self.lora_right_weight = nn.Parameter(torch.zeros(\n",
    "            columns,\n",
    "            lora_dim))  # apply transpose so in forward we do not need to\n",
    "        self.lora_left_weight = nn.Parameter(torch.zeros(lora_dim, rows))\n",
    "        self.lora_scaling = lora_scaling / lora_dim\n",
    "\n",
    "        if lora_droppout > 0:\n",
    "            self.lora_dropout = nn.Dropout(lora_droppout)\n",
    "        else:\n",
    "            self.lora_dropout = nn.Identity()\n",
    "\n",
    "        self.reset_parameters()\n",
    "        # disable the original weight gradient\n",
    "        self.weight.requires_grad = False\n",
    "        # fuse LoRA to the original weight\n",
    "        self.fuse_lora = False\n",
    "\n",
    "    def eval(self):\n",
    "        self.lora_dropout.eval()\n",
    "\n",
    "    #   self.fuse_lora_weight()\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        self.lora_dropout.train(mode)\n",
    "        # self.unfuse_lora_weight()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.lora_right_weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_left_weight)\n",
    "\n",
    "    def fuse_lora_weight(self):\n",
    "        if not self.fuse_lora:\n",
    "            self.weight.data += self.lora_scaling * torch.matmul(\n",
    "                self.lora_left_weight.t(), self.lora_right_weight.t())\n",
    "        self.fuse_lora = True\n",
    "\n",
    "    def unfuse_lora_weight(self):\n",
    "        if self.fuse_lora:\n",
    "            self.weight.data -= self.lora_scaling * torch.matmul(\n",
    "                self.lora_left_weight.t(), self.lora_right_weight.t())\n",
    "        self.fuse_lora = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.fuse_lora:\n",
    "            return F.linear(input, self.weight, self.bias)\n",
    "        else:\n",
    "            return F.linear(\n",
    "                input, self.weight,\n",
    "                self.bias) + (self.lora_dropout(input) @ self.lora_right_weight\n",
    "                              @ self.lora_left_weight) * self.lora_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from deepspeed.compression.helper import recursive_getattr, recursive_setattr\n",
    "\n",
    "def convert_linear_layer_to_lora(model,\n",
    "                                 part_module_name,\n",
    "                                 lora_dim=0,\n",
    "                                 lora_scaling=1,\n",
    "                                 lora_droppout=0):\n",
    "    replace_name = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and part_module_name in name:\n",
    "            replace_name.append(name)\n",
    "    print(replace_name)\n",
    "    for name in replace_name:\n",
    "        module = recursive_getattr(model, name)\n",
    "        tmp = LinearLayer_LoRA(\n",
    "            module.weight, lora_dim, lora_scaling, lora_droppout,\n",
    "            module.bias).to(module.weight.device).to(module.weight.dtype)\n",
    "        recursive_setattr(model, name, tmp)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.decoder.layers.0.self_attn.k_proj', 'model.decoder.layers.0.self_attn.v_proj', 'model.decoder.layers.0.self_attn.q_proj', 'model.decoder.layers.0.self_attn.out_proj', 'model.decoder.layers.0.fc1', 'model.decoder.layers.0.fc2', 'model.decoder.layers.1.self_attn.k_proj', 'model.decoder.layers.1.self_attn.v_proj', 'model.decoder.layers.1.self_attn.q_proj', 'model.decoder.layers.1.self_attn.out_proj', 'model.decoder.layers.1.fc1', 'model.decoder.layers.1.fc2', 'model.decoder.layers.2.self_attn.k_proj', 'model.decoder.layers.2.self_attn.v_proj', 'model.decoder.layers.2.self_attn.q_proj', 'model.decoder.layers.2.self_attn.out_proj', 'model.decoder.layers.2.fc1', 'model.decoder.layers.2.fc2', 'model.decoder.layers.3.self_attn.k_proj', 'model.decoder.layers.3.self_attn.v_proj', 'model.decoder.layers.3.self_attn.q_proj', 'model.decoder.layers.3.self_attn.out_proj', 'model.decoder.layers.3.fc1', 'model.decoder.layers.3.fc2', 'model.decoder.layers.4.self_attn.k_proj', 'model.decoder.layers.4.self_attn.v_proj', 'model.decoder.layers.4.self_attn.q_proj', 'model.decoder.layers.4.self_attn.out_proj', 'model.decoder.layers.4.fc1', 'model.decoder.layers.4.fc2', 'model.decoder.layers.5.self_attn.k_proj', 'model.decoder.layers.5.self_attn.v_proj', 'model.decoder.layers.5.self_attn.q_proj', 'model.decoder.layers.5.self_attn.out_proj', 'model.decoder.layers.5.fc1', 'model.decoder.layers.5.fc2', 'model.decoder.layers.6.self_attn.k_proj', 'model.decoder.layers.6.self_attn.v_proj', 'model.decoder.layers.6.self_attn.q_proj', 'model.decoder.layers.6.self_attn.out_proj', 'model.decoder.layers.6.fc1', 'model.decoder.layers.6.fc2', 'model.decoder.layers.7.self_attn.k_proj', 'model.decoder.layers.7.self_attn.v_proj', 'model.decoder.layers.7.self_attn.q_proj', 'model.decoder.layers.7.self_attn.out_proj', 'model.decoder.layers.7.fc1', 'model.decoder.layers.7.fc2', 'model.decoder.layers.8.self_attn.k_proj', 'model.decoder.layers.8.self_attn.v_proj', 'model.decoder.layers.8.self_attn.q_proj', 'model.decoder.layers.8.self_attn.out_proj', 'model.decoder.layers.8.fc1', 'model.decoder.layers.8.fc2', 'model.decoder.layers.9.self_attn.k_proj', 'model.decoder.layers.9.self_attn.v_proj', 'model.decoder.layers.9.self_attn.q_proj', 'model.decoder.layers.9.self_attn.out_proj', 'model.decoder.layers.9.fc1', 'model.decoder.layers.9.fc2', 'model.decoder.layers.10.self_attn.k_proj', 'model.decoder.layers.10.self_attn.v_proj', 'model.decoder.layers.10.self_attn.q_proj', 'model.decoder.layers.10.self_attn.out_proj', 'model.decoder.layers.10.fc1', 'model.decoder.layers.10.fc2', 'model.decoder.layers.11.self_attn.k_proj', 'model.decoder.layers.11.self_attn.v_proj', 'model.decoder.layers.11.self_attn.q_proj', 'model.decoder.layers.11.self_attn.out_proj', 'model.decoder.layers.11.fc1', 'model.decoder.layers.11.fc2', 'model.decoder.layers.12.self_attn.k_proj', 'model.decoder.layers.12.self_attn.v_proj', 'model.decoder.layers.12.self_attn.q_proj', 'model.decoder.layers.12.self_attn.out_proj', 'model.decoder.layers.12.fc1', 'model.decoder.layers.12.fc2', 'model.decoder.layers.13.self_attn.k_proj', 'model.decoder.layers.13.self_attn.v_proj', 'model.decoder.layers.13.self_attn.q_proj', 'model.decoder.layers.13.self_attn.out_proj', 'model.decoder.layers.13.fc1', 'model.decoder.layers.13.fc2', 'model.decoder.layers.14.self_attn.k_proj', 'model.decoder.layers.14.self_attn.v_proj', 'model.decoder.layers.14.self_attn.q_proj', 'model.decoder.layers.14.self_attn.out_proj', 'model.decoder.layers.14.fc1', 'model.decoder.layers.14.fc2', 'model.decoder.layers.15.self_attn.k_proj', 'model.decoder.layers.15.self_attn.v_proj', 'model.decoder.layers.15.self_attn.q_proj', 'model.decoder.layers.15.self_attn.out_proj', 'model.decoder.layers.15.fc1', 'model.decoder.layers.15.fc2', 'model.decoder.layers.16.self_attn.k_proj', 'model.decoder.layers.16.self_attn.v_proj', 'model.decoder.layers.16.self_attn.q_proj', 'model.decoder.layers.16.self_attn.out_proj', 'model.decoder.layers.16.fc1', 'model.decoder.layers.16.fc2', 'model.decoder.layers.17.self_attn.k_proj', 'model.decoder.layers.17.self_attn.v_proj', 'model.decoder.layers.17.self_attn.q_proj', 'model.decoder.layers.17.self_attn.out_proj', 'model.decoder.layers.17.fc1', 'model.decoder.layers.17.fc2', 'model.decoder.layers.18.self_attn.k_proj', 'model.decoder.layers.18.self_attn.v_proj', 'model.decoder.layers.18.self_attn.q_proj', 'model.decoder.layers.18.self_attn.out_proj', 'model.decoder.layers.18.fc1', 'model.decoder.layers.18.fc2', 'model.decoder.layers.19.self_attn.k_proj', 'model.decoder.layers.19.self_attn.v_proj', 'model.decoder.layers.19.self_attn.q_proj', 'model.decoder.layers.19.self_attn.out_proj', 'model.decoder.layers.19.fc1', 'model.decoder.layers.19.fc2', 'model.decoder.layers.20.self_attn.k_proj', 'model.decoder.layers.20.self_attn.v_proj', 'model.decoder.layers.20.self_attn.q_proj', 'model.decoder.layers.20.self_attn.out_proj', 'model.decoder.layers.20.fc1', 'model.decoder.layers.20.fc2', 'model.decoder.layers.21.self_attn.k_proj', 'model.decoder.layers.21.self_attn.v_proj', 'model.decoder.layers.21.self_attn.q_proj', 'model.decoder.layers.21.self_attn.out_proj', 'model.decoder.layers.21.fc1', 'model.decoder.layers.21.fc2', 'model.decoder.layers.22.self_attn.k_proj', 'model.decoder.layers.22.self_attn.v_proj', 'model.decoder.layers.22.self_attn.q_proj', 'model.decoder.layers.22.self_attn.out_proj', 'model.decoder.layers.22.fc1', 'model.decoder.layers.22.fc2', 'model.decoder.layers.23.self_attn.k_proj', 'model.decoder.layers.23.self_attn.v_proj', 'model.decoder.layers.23.self_attn.q_proj', 'model.decoder.layers.23.self_attn.out_proj', 'model.decoder.layers.23.fc1', 'model.decoder.layers.23.fc2']\n",
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
      "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
      "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): LinearLayer_LoRA(\n",
      "              (lora_dropout): Identity()\n",
      "            )\n",
      "            (v_proj): LinearLayer_LoRA(\n",
      "              (lora_dropout): Identity()\n",
      "            )\n",
      "            (q_proj): LinearLayer_LoRA(\n",
      "              (lora_dropout): Identity()\n",
      "            )\n",
      "            (out_proj): LinearLayer_LoRA(\n",
      "              (lora_dropout): Identity()\n",
      "            )\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LinearLayer_LoRA(\n",
      "            (lora_dropout): Identity()\n",
      "          )\n",
      "          (fc2): LinearLayer_LoRA(\n",
      "            (lora_dropout): Identity()\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = convert_linear_layer_to_lora(model=model, part_module_name='decoder.layers.',lora_dim=128)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the prompt dataset \n",
    "\n",
    "# 初始化变量\n",
    "local_rank = -1\n",
    "data_path = './DeepSpeed-Chat/download/Dahoas/rm-static'\n",
    "data_split = '2,4,4'\n",
    "output_path = './tmp/data_files/'\n",
    "train_phase = '1' # 没啥意义\n",
    "seed = 123 # 随便给个值\n",
    "tokenizer = tokenizer\n",
    "max_seq_length = 512\n",
    "end_of_conversation_token=\"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import torch\n",
    "\n",
    "def create_prompt_dataset(local_rank,\n",
    "                          data_path,\n",
    "                          data_split,\n",
    "                          output_path,\n",
    "                          train_phase,\n",
    "                          seed,\n",
    "                          tokenizer,\n",
    "                          max_seq_len,\n",
    "                          end_of_conversation_token=\"<|endoftext|>\",\n",
    "                          sft_only_data_path=[],\n",
    "                          reload=False):\n",
    "    \"\"\"\n",
    "    Creates the prompt dataset\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    fname = \"_\".join(data_path)\n",
    "    sft_cache_key = \"_\".join(sft_only_data_path)\n",
    "    tokenizer_name = tokenizer.init_kwargs[\"name_or_path\"].replace(\"/\", \"_\")\n",
    "    fname = f\"{fname}_split{data_split}_phase{train_phase}_seed{seed}_tokenizer{tokenizer_name}_seqlen{max_seq_len}_sft{sft_cache_key}\"\n",
    "    fname = \"_\".join(fname.split(\"/\"))\n",
    "    fname = hashlib.sha256(fname.encode()).hexdigest(\n",
    "    )  # hash the file name to avoid too long file name\n",
    "    train_fname = f\"{output_path}/traindata_{fname}.pt\"\n",
    "    eval_fname = f\"{output_path}/evaldata_{fname}.pt\"\n",
    "    \n",
    "    print(train_fname)\n",
    "    print(eval_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tmp/data_files//traindata_b8c90ef6e7ecd4d64f21e401eac09054cf9d084abc799cf0124886c853c3468f.pt\n",
      "./tmp/data_files//evaldata_b8c90ef6e7ecd4d64f21e401eac09054cf9d084abc799cf0124886c853c3468f.pt\n"
     ]
    }
   ],
   "source": [
    "create_prompt_dataset(local_rank, data_path, data_split, output_path, train_phase, seed, tokenizer, max_seq_len=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'response', 'chosen', 'rejected'],\n",
      "        num_rows: 76256\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'response', 'chosen', 'rejected'],\n",
      "        num_rows: 5103\n",
      "    })\n",
      "})\n",
      "{'prompt': ['\\n\\nHuman: Can you describe the steps to clean fingerprints and smudges from a laptop screen\\n\\nAssistant: Yes, certainly. To clean your screen, you first need to use a microfiber cloth or soft, damp cloth to gently wipe down the surface of the screen. Next, you’ll want to grab a soft, lint-free, microfiber cleaning cloth and gently rub it back and forth across the screen to remove fingerprints and smudges.\\n\\nHuman: Can I spray isopropyl alcohol onto the cloth and clean it that way?\\n\\nAssistant:', '\\n\\nHuman: What are some foods that are good for diabetics?\\n\\nAssistant: To be honest, some of these are better than others, and they’re a little more like opinions than facts. For example, many of the diets say to limit vegetables with high sugar content, and there’s some debate on the subject, as far as how much of these vegetables are actually bad for diabetics.\\n\\nHuman: Okay, any other advice?\\n\\nAssistant:', \"\\n\\nHuman: What animal would be the dominate life form on Earth if humans weren't here?\\n\\nAssistant: Most life on Earth would be taken over by bacteria and insects.\\n\\nHuman: What about birds? Could they ever come to dominate the Earth?\\n\\nAssistant:\", '\\n\\nHuman: How often are the Olympics?\\n\\nAssistant:', '\\n\\nHuman: Can I use car wax on my linoleum floor to make it shine?\\n\\nAssistant:'], 'response': [' Yes, you can do that to help the cloth pick up even more dirt from the screen. Be sure to always use a clean, soft cloth, not a piece of scratchy, roughened, or textured material, and make sure it’s lint-free.', ' What exactly are you asking? There’s a lot of different kinds of diabetic diets. I could try to recommend you some specific foods and recipes. I could help you look up any of the foods, and I could find recipes for them.', \" Possibly. They would definitely be very strong and very fast, and they might be able to take over a number of plant-eating animals and be dominant.  That's a guess, I'm not sure what exactly a dominant lifeform would be.\", ' It is estimated that the Olympics occur every four years.  Did that answer your question?', ' What sort of flooring is it?  This might be a complex problem, since there are many types of flooring and not all of them can be waxed in the same way.  Can you describe the flooring and its condition?'], 'chosen': [' Yes, you can do that to help the cloth pick up even more dirt from the screen. Be sure to always use a clean, soft cloth, not a piece of scratchy, roughened, or textured material, and make sure it’s lint-free.', ' What exactly are you asking? There’s a lot of different kinds of diabetic diets. I could try to recommend you some specific foods and recipes. I could help you look up any of the foods, and I could find recipes for them.', \" Possibly. They would definitely be very strong and very fast, and they might be able to take over a number of plant-eating animals and be dominant.  That's a guess, I'm not sure what exactly a dominant lifeform would be.\", ' It is estimated that the Olympics occur every four years.  Did that answer your question?', ' What sort of flooring is it?  This might be a complex problem, since there are many types of flooring and not all of them can be waxed in the same way.  Can you describe the flooring and its condition?'], 'rejected': [' Yes, you can spray it directly onto the cloth.', ' Sure, we’ve got information on common mistakes that diabetic patients make with their diets, and even some specific things to do when you eat out and on the go.  One thing that’s recommended in these articles is just to be very mindful of the timing of food intake.', ' Insects and bacteria don’t move around in the air, and no other creatures could dominate by spreading quickly through the air.  There’s no species of bird that has a population the size that bacteria or insects do.', ' For 2017, it was every four years.', ' I’m not sure. Can you tell me more about what you’re doing?']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "from datasets import load_from_disk, load_dataset\n",
    "raw_datasets = load_dataset(data_path)\n",
    "print(raw_datasets)\n",
    "print(raw_datasets['train'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 原始模型测试\n",
    "\n",
    "对模型opt-1.3b模型进行原始输出测试，后面训练完后再测试输出对比下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'s you looking for dinner tonight\n"
     ]
    }
   ],
   "source": [
    "# 测试1 需要使用通过集束搜索或者是贪婪搜索法生成\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./DeepSpeed-Chat/download/facebook/opt-350m/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./DeepSpeed-Chat/download/facebook/opt-350m/\")\n",
    "text = \"What are we having for dinner\"\n",
    "outputs = model(torch.tensor([tokenizer.encode(text)]))\n",
    "logits = outputs.logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "predicted_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"What are we having for dinner?\\nI'm having a steak and a salad.\\nI\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# huggingface\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "generator(\"What are we having for dinner?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
